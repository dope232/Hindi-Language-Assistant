{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10958662,"sourceType":"datasetVersion","datasetId":6817516}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-24T06:10:54.631134Z","iopub.execute_input":"2025-04-24T06:10:54.631307Z","iopub.status.idle":"2025-04-24T06:10:56.432898Z","shell.execute_reply.started":"2025-04-24T06:10:54.63129Z","shell.execute_reply":"2025-04-24T06:10:56.43209Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 1: Install Dependencies\n# Run this cell first to install all required packages\n!pip install numpy indic-nlp-library indic-transliteration langchain langchain_community faiss-cpu tqdm pandas\n!pip install sentence-transformers langchain_community tiktoken gradio\n!pip install torch torchvision torchaudio transformers\n!pip install accelerate bitsandbytes\n!pip install datasets\n!pip install langgraph langchain-core\n# Optional speech components - can be enabled later\n!pip install openai-whisper gTTS\n\n# Cell 2: Import Libraries and Define Constants\n# Run this cell to import all necessary dependencies\n\nimport numpy as np\nimport pandas as pd\nimport os\nimport json\nimport re\nimport gc\nimport torch\nfrom typing import TypedDict, List, Dict, Optional, Any, Union, Callable\nfrom pathlib import Path\nimport gradio as gr\nimport logging\nimport operator\n\n# Import LangChain and LangGraph components\nfrom langchain_core.messages import AIMessage, HumanMessage, SystemMessage, BaseMessage\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.language_models import BaseChatModel\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_core.outputs import ChatResult, ChatGeneration\nfrom langgraph.graph import StateGraph, END\n\n# Import components from your existing implementation\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForCausalLM, \n    BitsAndBytesConfig, \n    pipeline,\n    AutoModelForSequenceClassification\n)\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\nfrom langchain.schema import Document\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom datasets import Dataset\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n# Constants (updated for recommended models)\nHINDI_MODEL_NAME = \"Ryder99/Llama-3.2-3B-Instruct-Hindi\"  # Role-Playing Agent\nGENERATOR_MODEL_NAME = \"Triangle104/Unsloth_Llama-3.2-3B-Instruct-Q5_K_M-GGUF\"  # Scene & Character Generator\nEVALUATOR_MODEL_NAME = \"EpistemeAI/ReasoningCore-3B-T1_1\"  # Performance Critique Agent\n\n# Model loading configurations\nUSE_8BIT = False \nUSE_4BIT = True  # Use 4-bit for efficient loading\nMAX_NEW_TOKENS = 150\nTEMPERATURE = 0.2\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Updated embedding model for better compatibility with Llama tokenization\nEMBEDDING_MODEL = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"  # Better multilingual support\n\nRAG_DATA_PATH = \"/kaggle/input/rag-preprocessed-data/processed_hindi_dialogues.json\"\nFAISS_INDEX_PATH = \"/kaggle/working/hindi_dialogue_faiss_index\"\nENABLE_SPEECH = False  # Keep speech disabled to save memory\n\ndef setup_memory_management():\n    \"\"\"Configure global settings for better memory management\"\"\"\n    \n    # Configure PyTorch to release memory faster\n    torch.cuda.empty_cache()\n    \n    # Set up cache directory\n    cache_dir = \"/kaggle/working/model_cache\"\n    os.makedirs(cache_dir, exist_ok=True)\n    os.environ['TRANSFORMERS_CACHE'] = cache_dir\n    \n    # Configure environment variables for better memory handling\n    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'\n    \n    print(f\"Memory management configured. Using device: {DEVICE}\")\n    \n# Call this function at the end of Cell 2\nsetup_memory_management()\n\nprint(f\"Using device: {DEVICE}\")\nprint(\"Libraries imported and constants defined!\")\n\n# Cell 3: Define Basic Utility Functions\n# These are helper functions used throughout the system\n\ndef load_hindi_model():\n    \"\"\"Load the Hindi role-playing model\"\"\"\n    print(f\"Loading Hindi model {HINDI_MODEL_NAME}...\")\n    \n    # Configuration for 4-bit quantization\n    if USE_4BIT:\n        quantization_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_compute_dtype=torch.float16,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\"\n        )\n        \n        model = AutoModelForCausalLM.from_pretrained(\n            HINDI_MODEL_NAME,\n            quantization_config=quantization_config,\n            device_map=\"auto\",\n            torch_dtype=torch.float16\n        )\n    else:\n        # Standard FP16 loading\n        model = AutoModelForCausalLM.from_pretrained(\n            HINDI_MODEL_NAME,\n            device_map=\"auto\",\n            torch_dtype=torch.float16\n        )\n    \n    tokenizer = AutoTokenizer.from_pretrained(HINDI_MODEL_NAME)\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n        \n    # Force garbage collection\n    clear_gpu_memory()\n    \n    return model, tokenizer\n\ndef load_generator_model():\n    \"\"\"Load the scene and character generator model\"\"\"\n    print(f\"Loading generator model {GENERATOR_MODEL_NAME}...\")\n    \n    try:\n        # For standard models, not GGUF\n        if USE_4BIT:\n            quantization_config = BitsAndBytesConfig(\n                load_in_4bit=True,\n                bnb_4bit_compute_dtype=torch.float16,\n                bnb_4bit_use_double_quant=True,\n                bnb_4bit_quant_type=\"nf4\"\n            )\n            \n            model = AutoModelForCausalLM.from_pretrained(\n                \"meta-llama/Llama-3.2-3B-Instruct\",  # Using Llama-3.2 base as fallback\n                quantization_config=quantization_config,\n                device_map=\"auto\",\n                torch_dtype=torch.float16\n            )\n        else:\n            model = AutoModelForCausalLM.from_pretrained(\n                \"meta-llama/Llama-3.2-3B-Instruct\",\n                device_map=\"auto\",\n                torch_dtype=torch.float16\n            )\n        \n        tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n        if tokenizer.pad_token is None:\n            tokenizer.pad_token = tokenizer.eos_token\n    \n    except Exception as e:\n        print(f\"Error loading generator model: {e}\")\n        print(\"Falling back to smaller model...\")\n        \n        # Fallback to a smaller model if the primary one fails\n        if USE_4BIT:\n            quantization_config = BitsAndBytesConfig(\n                load_in_4bit=True,\n                bnb_4bit_compute_dtype=torch.float16,\n                bnb_4bit_use_double_quant=True,\n                bnb_4bit_quant_type=\"nf4\"\n            )\n            \n            model = AutoModelForCausalLM.from_pretrained(\n                \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",  # Much smaller fallback\n                quantization_config=quantization_config,\n                device_map=\"auto\",\n                torch_dtype=torch.float16\n            )\n        else:\n            model = AutoModelForCausalLM.from_pretrained(\n                \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n                device_map=\"auto\",\n                torch_dtype=torch.float16\n            )\n        \n        tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n        if tokenizer.pad_token is None:\n            tokenizer.pad_token = tokenizer.eos_token\n    \n    # Force garbage collection\n    clear_gpu_memory()\n    \n    return model, tokenizer\n\ndef load_evaluator_model():\n    \"\"\"Load the performance critique model\"\"\"\n    print(f\"Loading evaluator model {EVALUATOR_MODEL_NAME}...\")\n    \n    if USE_4BIT:\n        quantization_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_compute_dtype=torch.float16,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\"\n        )\n        \n        model = AutoModelForCausalLM.from_pretrained(\n            EVALUATOR_MODEL_NAME,\n            quantization_config=quantization_config,\n            device_map=\"auto\",\n            torch_dtype=torch.float16\n        )\n    else:\n        model = AutoModelForCausalLM.from_pretrained(\n            EVALUATOR_MODEL_NAME,\n            device_map=\"auto\",\n            torch_dtype=torch.float16\n        )\n    \n    tokenizer = AutoTokenizer.from_pretrained(EVALUATOR_MODEL_NAME)\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    \n    # Force garbage collection\n    clear_gpu_memory()\n    \n    return model, tokenizer\n\n# Also update the clear_gpu_memory function to be more aggressive\ndef clear_gpu_memory():\n    \"\"\"Clear GPU memory more aggressively to avoid OOM errors\"\"\"\n    if DEVICE == \"cuda\":\n        torch.cuda.empty_cache()\n        torch.cuda.synchronize()  # Wait for all CUDA operations to finish\n        gc.collect()  # Run full garbage collection\n\ndef is_hindi(text):\n    \"\"\"Check if the text contains Hindi (either in Devanagari or romanized)\"\"\"\n    # Check for Devanagari characters\n    devanagari_pattern = re.compile(r'[\\u0900-\\u097F]')\n    if devanagari_pattern.search(text):\n        return True\n\n    # Common Hindi romanized words\n    hindi_romanized_words = [\n        'namaste', 'dhanyavad', 'theek', 'haan', 'nahi', 'kya', 'aap', 'mai', 'tum',\n        'kitna', 'rupaye', 'paisa', 'khana', 'pani', 'chai', 'acha', 'bahut', 'thoda'\n    ]\n\n    text_lower = text.lower()\n    for word in hindi_romanized_words:\n        if word in text_lower:\n            return True\n\n    return False\n\ndef clean_response(response):\n    \"\"\"Clean up the response to ensure proper format and remove role confusion.\"\"\"\n    response = re.sub(r'(Shopkeeper|Waiter|Customer|Assistant):\\s*', '', response)\n    lines = response.split('\\n')\n    cleaned_lines = []\n    roman_line = \"\"\n    devanagari_line = \"\"\n    for line in lines:\n        if line.strip():\n            if not roman_line:\n                roman_line = line.strip()\n            elif not devanagari_line:\n                devanagari_line = line.strip()\n                break\n    if roman_line and devanagari_line:\n        return f\"{roman_line}\\n{devanagari_line}\"\n    return response\n\nprint(\"Utility functions defined!\")\n\n# Cell 4: RAG System Implementation\n# This is your existing RAG system with updates for efficiency\n\nclass HindiLearningRAG:\n    \"\"\"RAG system for retrieving Hindi dialogues, idioms, and examples.\"\"\"\n\n    def __init__(self, dummy_mode=False):\n        \"\"\"Initialize the RAG system with embeddings model.\"\"\"\n        self.dummy_mode = dummy_mode\n        if dummy_mode:\n            logger.info(\"Initializing dummy RAG system (no retrieval capabilities)\")\n            return\n\n        logger.info(f\"Initializing Hindi Learning RAG on {DEVICE}...\")\n        self.embeddings = None\n        self.vector_store = None\n        self.document_data = []\n        self.initialize_embeddings()\n        logger.info(\"RAG system initialized.\")\n\n    def initialize_embeddings(self):\n        \"\"\"Initialize the embeddings model.\"\"\"\n        if self.dummy_mode:\n            return\n\n        try:\n            self.embeddings = HuggingFaceEmbeddings(\n                model_name=EMBEDDING_MODEL,\n                model_kwargs={\"device\": DEVICE},\n                encode_kwargs={\"normalize_embeddings\": True}\n            )\n\n            logger.info(\"Embeddings model initialized.\")\n        except Exception as e:\n            logger.error(f\"Error initializing embeddings: {e}\")\n            logger.warning(\"Continuing in dummy mode (no retrieval capabilities)\")\n            self.dummy_mode = True\n\n    def load_documents(self, file_path=RAG_DATA_PATH):\n        \"\"\"Load documents from JSON file.\"\"\"\n        if self.dummy_mode:\n            return False\n\n        if not os.path.exists(file_path):\n            logger.warning(f\"Data file {file_path} not found. You need to load data first.\")\n            self.dummy_mode = True\n            return False\n\n        try:\n            logger.info(f\"Loading documents from {file_path}...\")\n            with open(file_path, 'r', encoding='utf-8') as f:\n                data = json.load(f)\n\n            self.document_data = data\n            logger.info(f\"Loaded {len(data)} documents.\")\n            return True\n        except Exception as e:\n            logger.error(f\"Error loading documents: {e}\")\n            self.dummy_mode = True\n            return False\n\n    def create_vector_store(self):\n        \"\"\"Create a FAISS vector store from loaded documents.\"\"\"\n        if self.dummy_mode or not self.document_data:\n            logger.warning(\"No documents loaded or in dummy mode. Cannot create vector store.\")\n            return False\n\n        try:\n            logger.info(\"Creating FAISS vector store...\")\n\n            documents = []\n            for item in self.document_data:\n                doc = Document(\n                    page_content=item[\"page_content\"],\n                    metadata=item[\"metadata\"]\n                )\n                documents.append(doc)\n\n            # Create vector store\n            self.vector_store = FAISS.from_documents(documents, self.embeddings)\n\n            logger.info(f\"Created vector store with {len(documents)} documents.\")\n\n            # Save index\n            if not os.path.exists(FAISS_INDEX_PATH):\n                os.makedirs(FAISS_INDEX_PATH)\n            self.vector_store.save_local(FAISS_INDEX_PATH)\n            logger.info(f\"Saved vector store to {FAISS_INDEX_PATH}\")\n\n            clear_gpu_memory()\n            return True\n        except Exception as e:\n            logger.error(f\"Error creating vector store: {e}\")\n            self.dummy_mode = True\n            return False\n\n    def load_vector_store(self, index_path=FAISS_INDEX_PATH):\n        \"\"\"Load a FAISS vector store from disk.\"\"\"\n        if self.dummy_mode:\n            return False\n\n        if not os.path.exists(index_path):\n            logger.warning(f\"Index path {index_path} not found. Create index first.\")\n            return False\n\n        try:\n            logger.info(f\"Loading vector store from {index_path}...\")\n            self.vector_store = FAISS.load_local(index_path, self.embeddings)\n            logger.info(\"Vector store loaded successfully.\")\n            return True\n        except Exception as e:\n            logger.error(f\"Error loading vector store: {e}\")\n            self.dummy_mode = True\n            return False\n\n    def retrieve_dialogue_examples(self, query, top_k=3, context_tags=None, emotion_tags=None):\n        \"\"\"Retrieve dialogue examples based on query and optional tags.\"\"\"\n        if self.dummy_mode:\n            return []\n\n        if not self.vector_store:\n            if not self.load_vector_store():\n                logger.warning(\"Vector store not available. Loading documents and creating index...\")\n                if self.load_documents() and self.create_vector_store():\n                    logger.info(\"Vector store created successfully.\")\n                else:\n                    self.dummy_mode = True\n                    return []\n\n        try:\n            logger.info(f\"Retrieving examples for query: {query}\")\n\n            # Get base retrieval results\n            retrieval_results = self.vector_store.similarity_search_with_score(query, k=top_k*2)\n\n            # Further filter by metadata if tags are provided\n            if context_tags or emotion_tags:\n                filtered_results = []\n                for doc, score in retrieval_results:\n                    metadata = doc.metadata\n\n                    # Check context tags\n                    context_match = True\n                    if context_tags:\n                        doc_context = set(metadata.get(\"context_tags\", []))\n                        query_context = set(context_tags)\n                        context_match = bool(doc_context.intersection(query_context))\n\n                    # Check emotion tags\n                    emotion_match = True\n                    if emotion_tags:\n                        doc_emotion = set(metadata.get(\"emotion_tags\", []))\n                        query_emotion = set(emotion_tags)\n                        emotion_match = bool(doc_emotion.intersection(query_emotion))\n\n                    if context_match and emotion_match:\n                        filtered_results.append((doc, score))\n\n                retrieval_results = filtered_results\n\n            # Sort by score and truncate\n            retrieval_results = sorted(retrieval_results, key=lambda x: x[1])[:top_k]\n\n            # Extract dialogue turns for each document\n            examples = []\n            for doc, score in retrieval_results:\n                example = {\n                    \"scene_description\": doc.metadata.get(\"scene_description\", \"\"),\n                    \"roman_dialogue\": doc.metadata.get(\"roman_dialogue\", \"\"),\n                    \"devanagari_dialogue\": doc.metadata.get(\"devanagari_dialogue\", \"\"),\n                    \"context_tags\": doc.metadata.get(\"context_tags\", []),\n                    \"emotion_tags\": doc.metadata.get(\"emotion_tags\", []),\n                    \"relevance_score\": float(score),\n                    \"dialogue_turns\": doc.metadata.get(\"dialogue_turns\", [])\n                }\n                examples.append(example)\n\n            logger.info(f\"Retrieved {len(examples)} examples.\")\n            return examples\n        except Exception as e:\n            logger.error(f\"Error retrieving examples: {e}\")\n            return []\n\n    def get_hindi_phrases_for_context(self, context, top_k=3):\n        \"\"\"Get relevant Hindi phrases based on the context.\"\"\"\n        if self.dummy_mode:\n            # Return default phrases for common scenarios\n            market_phrases = [\n                {\"phrase\": \"Kitne ka hai?\", \"meaning\": \"How much is it?\", \"devanagari\": \"कितने का है?\"},\n                {\"phrase\": \"Thoda kam kar dijiye\", \"meaning\": \"Please reduce it a little\", \"devanagari\": \"थोड़ा कम कर दीजिए\"},\n                {\"phrase\": \"Badhiya maal hai\", \"meaning\": \"It's good quality\", \"devanagari\": \"बढ़िया माल है\"}\n            ]\n\n            restaurant_phrases = [\n                {\"phrase\": \"Menu dikha dijiye\", \"meaning\": \"Please show me the menu\", \"devanagari\": \"मेनू दिखा दीजिए\"},\n                {\"phrase\": \"Thoda teekha hai\", \"meaning\": \"It's a bit spicy\", \"devanagari\": \"थोड़ा तीखा है\"},\n                {\"phrase\": \"Bill le aayiye\", \"meaning\": \"Please bring the bill\", \"devanagari\": \"बिल ले आइए\"}\n            ]\n\n            hotel_phrases = [\n                {\"phrase\": \"Kamra saaf hai?\", \"meaning\": \"Is the room clean?\", \"devanagari\": \"कमरा साफ है?\"},\n                {\"phrase\": \"Checkout ka samay kya hai?\", \"meaning\": \"What is the checkout time?\", \"devanagari\": \"चेकआउट का समय क्या है?\"},\n                {\"phrase\": \"WiFi password kya hai?\", \"meaning\": \"What is the WiFi password?\", \"devanagari\": \"वाईफाई पासवर्ड क्या है?\"}\n            ]\n\n            if \"market\" in context.lower():\n                return market_phrases[:top_k]\n            elif \"restaurant\" in context.lower():\n                return restaurant_phrases[:top_k]\n            elif \"hotel\" in context.lower():\n                return hotel_phrases[:top_k]\n            else:\n                return market_phrases[:top_k]  # Default to market\n\n        # If RAG is available, extract phrases from retrieved examples\n        examples = self.retrieve_dialogue_examples(context, top_k=top_k)\n\n        phrases = []\n        for example in examples:\n            dialogue_turns = example.get(\"dialogue_turns\", [])\n\n            # Extract short phrases from dialogue turns\n            for turn in dialogue_turns:\n                text_roman = turn.get(\"text_roman\", \"\")\n                text_devanagari = turn.get(\"text_devanagari\", \"\")\n\n                # Look for short phrases (3-5 words)\n                words = text_roman.split()\n                if 3 <= len(words) <= 10:\n                    phrases.append({\n                        \"phrase\": text_roman,\n                        \"devanagari\": text_devanagari,\n                        \"meaning\": \"\"  # We would need translation for this\n                    })\n\n        # Return unique phrases, limited to top_k\n        unique_phrases = []\n        seen_phrases = set()\n\n        for phrase in phrases:\n            if phrase[\"phrase\"] not in seen_phrases:\n                seen_phrases.add(phrase[\"phrase\"])\n                unique_phrases.append(phrase)\n\n                if len(unique_phrases) >= top_k:\n                    break\n\n        # If we don't have enough phrases, add default ones\n        if len(unique_phrases) < top_k:\n            default_phrases = [\n                {\"phrase\": \"Kitne ka hai?\", \"meaning\": \"How much is it?\", \"devanagari\": \"कितने का है?\"},\n                {\"phrase\": \"Thoda kam kar dijiye\", \"meaning\": \"Please reduce it a little\", \"devanagari\": \"थोड़ा कम कर दीजिए\"},\n                {\"phrase\": \"Badhiya maal hai\", \"meaning\": \"It's good quality\", \"devanagari\": \"बढ़िया माल है\"}\n            ]\n\n            for phrase in default_phrases:\n                if phrase[\"phrase\"] not in seen_phrases and len(unique_phrases) < top_k:\n                    seen_phrases.add(phrase[\"phrase\"])\n                    unique_phrases.append(phrase)\n\n        return unique_phrases\n\nprint(\"RAG system defined!\")\n\n# Cell 5: LLM Model Wrappers\n# Define wrappers for local models to use with LangChain/LangGraph\n\n# Cell 5: LLM Model Wrappers\n# Define wrappers for local models to use with LangChain/LangGraph\n\nclass LocalModelWrapper(BaseChatModel):\n    \"\"\"Wrapper for local models to make them compatible with LangChain\"\"\"\n    \n    def __init__(self, model, tokenizer, model_name=\"local-model\"):\n        # Important: Set these as instance variables before calling super().__init__()\n        self._model = model\n        self._tokenizer = tokenizer\n        self._model_name = model_name\n        # Call super().__init__() after setting instance variables\n        super().__init__()\n        \n    def _generate(self, messages, stop=None, run_manager=None, **kwargs):\n        \"\"\"Generate text based on messages\"\"\"\n        try:\n            # Format the prompt based on messages\n            formatted_prompt = self._format_messages_to_prompt(messages)\n            \n            # Tokenize and generate\n            input_ids = self._tokenizer(formatted_prompt, return_tensors=\"pt\").input_ids.to(self._model.device)\n            \n            # Set generation parameters with good defaults\n            gen_kwargs = {\n                \"max_new_tokens\": kwargs.get(\"max_new_tokens\", 512),\n                \"temperature\": kwargs.get(\"temperature\", 0.7),\n                \"top_p\": kwargs.get(\"top_p\", 0.9),\n                \"repetition_penalty\": kwargs.get(\"repetition_penalty\", 1.1),\n                \"do_sample\": True\n            }\n            \n            # Generate text\n            with torch.no_grad():\n                output_ids = self._model.generate(input_ids, **gen_kwargs)\n            \n            # Decode the generated text\n            generated_text = self._tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)\n            \n            # Create and return ChatResult\n            generation = ChatGeneration(message=AIMessage(content=generated_text))\n            return ChatResult(generations=[generation])\n            \n        except Exception as e:\n            logger.error(f\"Error in LocalModelWrapper._generate: {e}\")\n            # Return a simple error message if generation fails\n            generation = ChatGeneration(message=AIMessage(content=f\"Error generating response: {str(e)}\"))\n            return ChatResult(generations=[generation])\n    \n    def _format_messages_to_prompt(self, messages):\n        \"\"\"Format a list of messages into a prompt for the model.\"\"\"\n        prompt_parts = []\n        \n        for message in messages:\n            if isinstance(message, SystemMessage):\n                prompt_parts.append(f\"System: {message.content}\")\n            elif isinstance(message, HumanMessage):\n                prompt_parts.append(f\"Human: {message.content}\")\n            elif isinstance(message, AIMessage):\n                prompt_parts.append(f\"Assistant: {message.content}\")\n            else:\n                prompt_parts.append(f\"{message.type}: {message.content}\")\n        \n        return \"\\n\\n\".join(prompt_parts) + \"\\n\\nAssistant:\"\n    \n    @property\n    def _llm_type(self):\n        \"\"\"Return the type identifier for this LLM.\"\"\"\n        return f\"local-llm-{self._model_name}\"\n    \n    @property\n    def _identifying_params(self):\n        \"\"\"Return identifying parameters for this LLM.\"\"\"\n        return {\"model_name\": self._model_name}\n\nclass TemplateModelWrapper(BaseChatModel):\n    \"\"\"Wrapper for local models that require specific formatting\"\"\"\n    \n    def __init__(self, model, tokenizer, model_name=\"template-model\"):\n        # Important: Set these as instance variables before calling super().__init__()\n        self._model = model\n        self._tokenizer = tokenizer\n        self._model_name = model_name\n        # Call super().__init__() after setting instance variables\n        super().__init__()\n        \n    def _generate(self, messages, stop=None, run_manager=None, **kwargs):\n        \"\"\"Generate text based on messages using a template format\"\"\"\n        try:\n            # Format the prompt based on messages\n            formatted_prompt = self._format_messages_to_prompt(messages)\n            \n            # Tokenize and generate\n            input_ids = self._tokenizer(formatted_prompt, return_tensors=\"pt\").input_ids.to(self._model.device)\n            \n            # Set generation parameters with good defaults\n            gen_kwargs = {\n                \"max_new_tokens\": kwargs.get(\"max_new_tokens\", 512),\n                \"temperature\": kwargs.get(\"temperature\", 0.7),\n                \"top_p\": kwargs.get(\"top_p\", 0.9),\n                \"repetition_penalty\": kwargs.get(\"repetition_penalty\", 1.1),\n                \"do_sample\": True\n            }\n            \n            # Generate text\n            with torch.no_grad():\n                output_ids = self._model.generate(input_ids, **gen_kwargs)\n            \n            # Decode the generated text\n            generated_text = self._tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)\n            \n            # Create and return ChatResult\n            generation = ChatGeneration(message=AIMessage(content=generated_text))\n            return ChatResult(generations=[generation])\n            \n        except Exception as e:\n            logger.error(f\"Error in TemplateModelWrapper._generate: {e}\")\n            # Return a simple error message if generation fails\n            generation = ChatGeneration(message=AIMessage(content=f\"Error generating response: {str(e)}\"))\n            return ChatResult(generations=[generation])\n    \n    def _format_messages_to_prompt(self, messages):\n        \"\"\"Format messages using a specific template for generator model.\"\"\"\n        system_message = \"\"\n        user_messages = []\n        \n        # Extract system and user messages\n        for message in messages:\n            if isinstance(message, SystemMessage):\n                system_message = message.content\n            elif isinstance(message, HumanMessage):\n                user_messages.append(message.content)\n        \n        # Combine all user messages if there are multiple\n        user_content = \"\\n\".join(user_messages) if user_messages else \"\"\n        \n        # Format using LLama 3 style instruction template\n        if system_message:\n            prompt = f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n{system_message}<|eot_id|>\\n\\n<|start_header_id|>user<|end_header_id|>\\n{user_content}<|eot_id|>\\n\\n<|start_header_id|>assistant<|end_header_id|>\\n\"\n        else:\n            prompt = f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n{user_content}<|eot_id|>\\n\\n<|start_header_id|>assistant<|end_header_id|>\\n\"\n            \n        return prompt\n    \n    @property\n    def _llm_type(self):\n        \"\"\"Return the type identifier for this LLM.\"\"\"\n        return f\"template-llm-{self._model_name}\"\n    \n    @property\n    def _identifying_params(self):\n        \"\"\"Return identifying parameters for this LLM.\"\"\"\n        return {\"model_name\": self._model_name}\n\nprint(\"LLM Model Wrappers defined!\")\n\n\n# Cell 6: Define LangGraph Components\n# COMPLETE UPDATED LANGGRAPH STRUCTURE\n# This should replace your entire Cell 6 with the LangGraph components\nclass ConversationState(TypedDict):\n    \"\"\"State structure for the conversation flow\"\"\"\n    messages: List[BaseMessage]\n    scenario: Optional[Dict[str, Any]]\n    evaluation: Optional[Dict[str, Any]]\n    dialogue_history: List[Dict[str, str]]\n    current_stage: str\n    user_input: Optional[str]\n    user_used_hindi: bool\n    proficiency_level: str  # beginner, intermediate, advanced\n\n# Custom prompt templates for different models\nGENERATOR_SYSTEM_PROMPT = \"\"\"You are a creative Hindi language learning scenario generator.\n        \nCreate a realistic and practical scenario for Hindi conversation practice at the {proficiency_level} level.\nInclude the following in your response:\n1. scenario_type: A category like \"restaurant\", \"market\", \"transportation\", \"hotel\", etc.\n2. scenario_title: A concise title for this scenario\n3. scenario_description: A brief description (2-3 sentences) of the setting\n4. character_role: The role the AI assistant will play (e.g., \"waiter\", \"shopkeeper\")\n5. user_role: The role the learner will play (usually \"customer\", \"traveler\", etc.)\n6. goals: 3-5 simple conversation goals for the learner to accomplish in this scenario\n7. key_vocabulary: 5-8 key Hindi words/phrases relevant to this scenario with both Roman and Devanagari script\n8. first_line: An opening line for the conversation (what the assistant should say first) in both Roman and Devanagari script\n\nFormat your response as a JSON object. Use this exact format:\n{{\n  \"scenario_type\": \"restaurant\",\n  \"scenario_title\": \"Ordering Food at a Restaurant\", \n  \"scenario_description\": \"You are at a local restaurant in Delhi...\",\n  \"character_role\": \"waiter\",\n  \"user_role\": \"customer\",\n  \"goals\": [\"Order a main dish\", \"Ask about spiciness\", \"Request the bill\"],\n  \"key_vocabulary\": [\n    {{\"roman\": \"menu\", \"devanagari\": \"मेनू\", \"meaning\": \"menu\"}},\n    {{\"roman\": \"khana\", \"devanagari\": \"खाना\", \"meaning\": \"food\"}},\n    {{\"roman\": \"bill\", \"devanagari\": \"बिल\", \"meaning\": \"bill\"}}\n  ],\n  \"first_line\": {{\n    \"roman\": \"Namaste ji, kya khaayenge aap?\",\n    \"devanagari\": \"नमस्ते जी, क्या खाएंगे आप?\"\n  }}\n}}\n\"\"\"\n\nEVALUATOR_SYSTEM_PROMPT = \"\"\"You are a Hindi language learning evaluator. Your task is to analyze the conversation between a learner (practicing Hindi at {proficiency_level} level) and an AI tutor, and provide detailed, constructive feedback.\n\nContext: The scenario was a {scenario_type} conversation where the AI played a {character_role} and the learner played a {user_role}.\n\n<reasoning>\nAnalyze the following aspects and rate each on a scale of 1-5:\n1. Hindi Usage: To what extent did the learner use Hindi (vs English)? Recognize any Hindi words/phrases they used.\n2. Cultural Appropriateness: Did their responses make sense in an Indian {scenario_type} context?\n3. Goal Achievement: The scenario had these goals: {goals}. How well did the learner accomplish them?\n4. Areas for Improvement: Specific suggestions for vocabulary, phrases, or cultural elements to incorporate next time.\n</reasoning>\n\n<answer>\nFormat your score sections like this example:\nHindi Usage: 3/5\nCultural Appropriateness: 4/5\nGoal Achievement: 3/5  \nOverall Rating: 3/5\n\nProvide encouragement and specific examples from their conversation in your feedback.\n</answer>\n\"\"\"\n\n# Add this function for fallback scenarios\ndef get_fallback_scenario(proficiency_level=\"beginner\"):\n    \"\"\"Generate a predefined scenario based on proficiency level if the model fails\"\"\"\n    \n    # Beginner scenario - Restaurant\n    if proficiency_level == \"beginner\":\n        return {\n            \"scenario_type\": \"restaurant\",\n            \"scenario_title\": \"Ordering Food at a Restaurant\", \n            \"scenario_description\": \"You are at a local Indian restaurant and want to order a meal. The waiter approaches your table.\",\n            \"character_role\": \"waiter\",\n            \"user_role\": \"customer\",\n            \"goals\": [\"Order a main dish\", \"Ask about spiciness\", \"Request the bill\"],\n            \"key_vocabulary\": [\n                {\"roman\": \"menu\", \"devanagari\": \"मेनू\", \"meaning\": \"menu\"},\n                {\"roman\": \"khana\", \"devanagari\": \"खाना\", \"meaning\": \"food\"},\n                {\"roman\": \"bill\", \"devanagari\": \"बिल\", \"meaning\": \"bill\"},\n                {\"roman\": \"paani\", \"devanagari\": \"पानी\", \"meaning\": \"water\"},\n                {\"roman\": \"teekha\", \"devanagari\": \"तीखा\", \"meaning\": \"spicy\"}\n            ],\n            \"first_line\": {\n                \"roman\": \"Namaste ji, kya khaayenge aap?\",\n                \"devanagari\": \"नमस्ते जी, क्या खाएंगे आप?\"\n            }\n        }\n    \n    # Intermediate scenario - Market\n    elif proficiency_level == \"intermediate\":\n        return {\n            \"scenario_type\": \"market\",\n            \"scenario_title\": \"Shopping at the Vegetable Market\", \n            \"scenario_description\": \"You are at a busy local market wanting to buy fresh vegetables. A shopkeeper is ready to assist you.\",\n            \"character_role\": \"shopkeeper\",\n            \"user_role\": \"customer\",\n            \"goals\": [\"Ask about prices\", \"Negotiate a discount\", \"Buy multiple items\"],\n            \"key_vocabulary\": [\n                {\"roman\": \"sabzi\", \"devanagari\": \"सब्ज़ी\", \"meaning\": \"vegetable\"},\n                {\"roman\": \"kitne ka hai\", \"devanagari\": \"कितने का है\", \"meaning\": \"how much is it\"},\n                {\"roman\": \"kilo\", \"devanagari\": \"किलो\", \"meaning\": \"kilogram\"},\n                {\"roman\": \"dam\", \"devanagari\": \"दाम\", \"meaning\": \"price\"},\n                {\"roman\": \"sasta\", \"devanagari\": \"सस्ता\", \"meaning\": \"cheap\"}\n            ],\n            \"first_line\": {\n                \"roman\": \"Aaiye ji, kya chahiye aapko?\",\n                \"devanagari\": \"आइये जी, क्या चाहिए आपको?\"\n            }\n        }\n    \n    # Advanced scenario - Hotel\n    else:  # advanced\n        return {\n            \"scenario_type\": \"hotel\",\n            \"scenario_title\": \"Checking into a Hotel\", \n            \"scenario_description\": \"You've arrived at a hotel in Delhi and need to check in. The receptionist is waiting to help you.\",\n            \"character_role\": \"receptionist\",\n            \"user_role\": \"guest\",\n            \"goals\": [\"Complete check-in process\", \"Ask about amenities\", \"Request a wake-up call\"],\n            \"key_vocabulary\": [\n                {\"roman\": \"kamra\", \"devanagari\": \"कमरा\", \"meaning\": \"room\"},\n                {\"roman\": \"booking\", \"devanagari\": \"बुकिंग\", \"meaning\": \"booking\"},\n                {\"roman\": \"checkout\", \"devanagari\": \"चेकआउट\", \"meaning\": \"checkout\"},\n                {\"roman\": \"wifi\", \"devanagari\": \"वाईफाई\", \"meaning\": \"WiFi\"},\n                {\"roman\": \"chaabi\", \"devanagari\": \"चाबी\", \"meaning\": \"key\"}\n            ],\n            \"first_line\": {\n                \"roman\": \"Namaste ji, swagat hai aapka. Kaise madad kar sakta hoon?\",\n                \"devanagari\": \"नमस्ते जी, स्वागत है आपका। कैसे मदद कर सकता हूं?\"\n            }\n        }\n\nclass LangGraphAgents:\n    \"\"\"Define and coordinate LangGraph agents for Hindi learning system\"\"\"\n    \n    def __init__(self, generator_llm, hindi_model, hindi_tokenizer, evaluator_llm, rag_system):\n        self.generator_llm = generator_llm  # For scenario generation\n        self.hindi_model = hindi_model      # For conversation\n        self.hindi_tokenizer = hindi_tokenizer\n        self.evaluator_llm = evaluator_llm  # For evaluation\n        self.rag_system = rag_system\n        \n    def scenario_generator_agent(self, state: ConversationState) -> ConversationState:\n        \"\"\"Generate a new conversation scenario based on proficiency level with better error handling\"\"\"\n        messages = state[\"messages\"]\n        proficiency_level = state[\"proficiency_level\"]\n        \n        # Create system prompt for scenario generator with custom template for this model\n        system_prompt = GENERATOR_SYSTEM_PROMPT.format(proficiency_level=proficiency_level)\n        \n        # Get scenario from the generator LLM\n        scenario_prompt = ChatPromptTemplate.from_messages([\n            (\"system\", system_prompt),\n            MessagesPlaceholder(variable_name=\"messages\")\n        ])\n        \n        # Create a new state to return (avoid modifying the input state directly)\n        new_state = state.copy()\n        \n        # Generate scenario\n        try:\n            # First check if generator LLM is available\n            if not hasattr(self, 'generator_llm') or self.generator_llm is None:\n                logger.error(\"Generator LLM not available, using fallback scenario\")\n                raise ValueError(\"Generator LLM not available\")\n                \n            # Create scenario chain and invoke\n            scenario_chain = scenario_prompt | self.generator_llm\n            scenario_response = scenario_chain.invoke({\"messages\": messages})\n            logger.info(\"Generated scenario response\")\n            \n            # Parse the JSON response (handling potential errors)\n            try:\n                # Try to extract JSON from the response\n                content = scenario_response.content\n                logger.info(f\"Raw scenario content: {content[:100]}...\")\n                \n                # Find JSON-like structure\n                import re\n                json_pattern = r'```json\\s*([\\s\\S]*?)\\s*```|(\\{[\\s\\S]*\\})'\n                match = re.search(json_pattern, content)\n                \n                if match:\n                    json_str = match.group(1) or match.group(2)\n                    logger.info(f\"Found JSON structure: {json_str[:100]}...\")\n                    scenario_result = json.loads(json_str)\n                else:\n                    # If no match with code blocks, try to parse the whole response\n                    logger.info(\"No JSON block found, attempting to parse entire content\")\n                    scenario_result = json.loads(content)\n                    \n                # Validate required fields\n                required_fields = [\"scenario_type\", \"scenario_title\", \"scenario_description\", \n                                \"character_role\", \"user_role\", \"goals\", \"key_vocabulary\", \"first_line\"]\n                \n                for field in required_fields:\n                    if field not in scenario_result:\n                        logger.warning(f\"Missing required field: {field}\")\n                        raise ValueError(f\"Missing required field: {field}\")\n                        \n            except Exception as e:\n                logger.error(f\"Error parsing scenario JSON: {e}\")\n                # Use fallback scenario\n                logger.info(f\"Using fallback scenario for {proficiency_level}\")\n                scenario_result = get_fallback_scenario(proficiency_level)\n                \n        except Exception as e:\n            logger.error(f\"Complete failure in scenario generation: {e}\")\n            # Use fallback scenario\n            logger.info(f\"Using fallback scenario for {proficiency_level}\")\n            scenario_result = get_fallback_scenario(proficiency_level)\n        \n        # Update state with new scenario\n        new_state[\"scenario\"] = scenario_result\n        new_state[\"current_stage\"] = \"language_tutor\"\n        \n        # Add system message to inform about the scenario\n        system_message = SystemMessage(content=f\"A new scenario has been generated: {scenario_result['scenario_title']}\")\n        \n        # Create the assistant's first message from the scenario\n        first_line_roman = scenario_result[\"first_line\"].get(\"roman\", \"Namaste!\")\n        first_line_devanagari = scenario_result[\"first_line\"].get(\"devanagari\", \"नमस्ते!\")\n        first_message = AIMessage(content=f\"{first_line_roman}\\n{first_line_devanagari}\")\n        \n        new_state[\"messages\"] = messages + [system_message, first_message]\n        new_state[\"dialogue_history\"] = [{\n            \"role\": \"assistant\",\n            \"content\": f\"{first_line_roman}\\n{first_line_devanagari}\"\n        }]\n        \n        return new_state\n        \n    def language_tutor_agent(self, state: ConversationState) -> ConversationState:\n        \"\"\"Process user input and generate Hindi tutor responses with improved error handling\"\"\"\n        try:\n            messages = state[\"messages\"]\n            scenario = state[\"scenario\"]\n            dialogue_history = state[\"dialogue_history\"]\n            user_input = state[\"user_input\"]\n            \n            # Check if user is using Hindi\n            user_used_hindi = is_hindi(user_input)\n            new_state = state.copy()\n            new_state[\"user_used_hindi\"] = user_used_hindi\n            \n            # Add user message to dialogue history\n            dialogue_history.append({\n                \"role\": \"user\",\n                \"content\": user_input\n            })\n            \n            # Create a system prompt for the Hindi model based on scenario\n            character_role = scenario[\"character_role\"]\n            scenario_type = scenario[\"scenario_type\"]\n            \n            # Get relevant RAG examples if available\n            rag_examples = []\n            if self.rag_system and not self.rag_system.dummy_mode:\n                context_tags = [scenario_type]\n                rag_examples = self.rag_system.retrieve_dialogue_examples(\n                    query=user_input,\n                    top_k=2,\n                    context_tags=context_tags\n                )\n            \n            # Create role-locked prompt\n            system_prompt = f\"\"\"You are a Hindi language tutor demonstrating ONLY the {character_role} role in a {scenario_type} conversation.\n\n    CRITICAL ROLE INSTRUCTIONS:\n    1. You ONLY play the {character_role} - NEVER respond as the customer/user.\n    2. The human user plays the {scenario[\"user_role\"]} role.\n    3. NEVER continue the conversation as the customer.\n    4. NEVER put \"Customer:\" or similar labels in your responses.\n\n    FORMAT REQUIREMENTS:\n    1. First line: Response in Roman Hindi (1-2 sentences)\n    2. Second line: Same response in Devanagari script\n    3. NOTHING ELSE.\n\n    CONTENT GUIDELINES:\n    1. Keep responses SHORT and PRACTICAL.\n    2. Use authentic, everyday Hindi appropriate for a {scenario_type} setting.\n    3. Match the user's proficiency level with appropriate vocabulary and complexity.\n    4. Use REALISTIC Hindi that would be spoken in a real {scenario_type}.\n    \"\"\"\n\n            if user_used_hindi:\n                system_prompt += \"\\nNOTE: The learner is responding in Hindi, which is excellent! Acknowledge their effort in your response.\"\n                \n            # Format RAG examples for the prompt if available\n            if rag_examples:\n                rag_content = \"\\n\\nREFERENCE EXAMPLES (use these for authentic Hindi expressions):\\n\"\n                for i, example in enumerate(rag_examples[:2]):\n                    turns = example.get(\"dialogue_turns\", [])\n                    if turns:\n                        rag_content += f\"Example {i+1}:\\n\"\n                        for j, turn in enumerate(turns[:3]):\n                            speaker = turn.get(\"speaker\", \"\")\n                            text = turn.get(\"text_roman\", \"\")\n                            rag_content += f\"{speaker}: {text}\\n\"\n                        rag_content += \"\\n\"\n                system_prompt += rag_content\n            \n            # Create a prompt for the Hindi model\n            messages_formatted = [\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": f\"The {scenario['user_role']} says: \\\"{user_input}\\\"\\n\\nRespond ONLY as the {character_role} in simple Hindi (both Roman and Devanagari). NEVER respond as the {scenario['user_role']}. Keep your response brief and practical.\"}\n            ]\n            \n            # Check if Hindi model is available\n            if not hasattr(self, 'hindi_model') or self.hindi_model is None:\n                raise ValueError(\"Hindi model not available\")\n                \n            if not hasattr(self, 'hindi_tokenizer') or self.hindi_tokenizer is None:\n                raise ValueError(\"Hindi tokenizer not available\")\n                \n            # Tokenize and generate\n            try:\n                input_ids = self.hindi_tokenizer.apply_chat_template(\n                    messages_formatted,\n                    add_generation_prompt=True,\n                    return_tensors=\"pt\"\n                ).to(self.hindi_model.device)\n                \n                outputs = self.hindi_model.generate(\n                    input_ids,\n                    max_new_tokens=100,\n                    do_sample=True,\n                    temperature=TEMPERATURE,\n                    repetition_penalty=1.3,\n                    eos_token_id=self.hindi_tokenizer.eos_token_id,\n                )\n                \n                tutor_response = self.hindi_tokenizer.decode(\n                    outputs[0][input_ids.shape[-1]:], \n                    skip_special_tokens=True\n                )\n            except Exception as e:\n                logger.error(f\"Error generating Hindi response: {e}\")\n                # Fallback response in case of generation error\n                tutor_response = \"Sorry, I couldn't generate a proper Hindi response.\\nक्षमा करें, मैं उचित हिंदी प्रतिक्रिया नहीं दे सका।\"\n            \n            # Clean the response to ensure proper format\n            tutor_response = clean_response(tutor_response)\n            \n            # Add tutor response to dialogue history\n            dialogue_history.append({\n                \"role\": \"assistant\",\n                \"content\": tutor_response\n            })\n            \n            # Update state\n            new_state[\"dialogue_history\"] = dialogue_history\n            new_state[\"messages\"] = messages + [HumanMessage(content=user_input), AIMessage(content=tutor_response)]\n            \n            # Determine next stage - if we've reached 5+ turns, check if user wants evaluation\n            if len(dialogue_history) >= 10 or \"evaluate\" in user_input.lower():\n                new_state[\"current_stage\"] = \"evaluator\"\n            else:\n                new_state[\"current_stage\"] = \"language_tutor\"\n                \n            # Clear user input\n            new_state[\"user_input\"] = None\n            \n            # Clear memory\n            clear_gpu_memory()\n            \n            return new_state\n            \n        except Exception as e:\n            logger.error(f\"Error in language_tutor_agent: {e}\")\n            # Create a recovery state to avoid system crash\n            new_state = state.copy()\n            error_message = AIMessage(content=f\"Sorry, I encountered an error processing your message. Let's try again.\\nक्षमा करें, मुझे आपके संदेश को प्रोसेस करने में त्रुटि मिली। फिर से प्रयास करें।\")\n            new_state[\"messages\"] = state[\"messages\"] + [HumanMessage(content=state[\"user_input\"]), error_message]\n            new_state[\"current_stage\"] = \"language_tutor\"\n            new_state[\"user_input\"] = None\n            return new_state\n        \n    def evaluator_agent(self, state: ConversationState) -> ConversationState:\n        \"\"\"Evaluate the conversation and provide feedback using the dedicated reasoning model\"\"\"\n        messages = state[\"messages\"]\n        scenario = state[\"scenario\"]\n        dialogue_history = state[\"dialogue_history\"]\n        proficiency_level = state[\"proficiency_level\"]\n        \n        # Create a system prompt for the evaluator with custom template for this model\n        system_prompt = EVALUATOR_SYSTEM_PROMPT.format(\n            proficiency_level=proficiency_level,\n            scenario_type=scenario[\"scenario_type\"],\n            character_role=scenario[\"character_role\"],\n            user_role=scenario[\"user_role\"],\n            goals=\", \".join(scenario[\"goals\"])\n        )\n        \n        # Extract the conversation history for the evaluator\n        conversation_text = \"Conversation transcript:\\n\"\n        for turn in dialogue_history:\n            role = \"Tutor\" if turn[\"role\"] == \"assistant\" else \"Learner\"\n            conversation_text += f\"{role}: {turn['content']}\\n\\n\"\n            \n        # Create the evaluation prompt\n        evaluation_prompt = ChatPromptTemplate.from_messages([\n            (\"system\", system_prompt),\n            (\"user\", conversation_text)\n        ])\n        \n        # Generate evaluation using the evaluator model\n        evaluation_chain = evaluation_prompt | self.evaluator_llm\n        evaluation_response = evaluation_chain.invoke({})\n        \n        # Parse the response to extract key information\n        evaluation_text = evaluation_response.content\n        \n        # Try to extract scores\n        import re\n        \n        # Look for scores in standard format and reasoning/answer blocks\n        def extract_score(pattern, text, default=3):\n            match = re.search(pattern, text)\n            if match:\n                try:\n                    return int(match.group(1))\n                except:\n                    return default\n            return default\n            \n        # Try to extract content from reasoning/answer blocks\n        reasoning_match = re.search(r'<reasoning>(.*?)</reasoning>', evaluation_text, re.DOTALL)\n        answer_match = re.search(r'<answer>(.*?)</answer>', evaluation_text, re.DOTALL)\n        \n        # If reasoning/answer blocks are found, use them to extract scores and format response\n        if reasoning_match and answer_match:\n            reasoning = reasoning_match.group(1).strip()\n            answer = answer_match.group(1).strip()\n            \n            # Extract scores from the answer block, which should be more structured\n            hindi_usage_score = extract_score(r'Hindi Usage:?\\s*(\\d+)/5', answer)\n            cultural_score = extract_score(r'Cultural Appropriateness:?\\s*(\\d+)/5', answer)\n            goal_score = extract_score(r'Goal Achievement:?\\s*(\\d+)/5', answer)\n            overall_score = extract_score(r'Overall Rating:?\\s*(\\d+)/5', answer)\n            \n            # Use answer as the main content, but include reasoning\n            evaluation_text = f\"{answer}\\n\\n**Detailed Analysis:**\\n{reasoning}\"\n        else:\n            # Standard extraction if no reasoning/answer blocks\n            hindi_usage_score = extract_score(r'Hindi Usage:?\\s*(\\d+)/5', evaluation_text)\n            cultural_score = extract_score(r'Cultural Appropriateness:?\\s*(\\d+)/5', evaluation_text)\n            goal_score = extract_score(r'Goal Achievement:?\\s*(\\d+)/5', evaluation_text)\n            overall_score = extract_score(r'Overall Rating:?\\s*(\\d+)/5', evaluation_text)\n        \n        # Create a structured evaluation result\n        evaluation_result = {\n            \"hindi_usage_score\": hindi_usage_score,\n            \"cultural_appropriateness_score\": cultural_score,\n            \"goal_achievement_score\": goal_score,\n            \"overall_score\": overall_score,\n            \"full_evaluation\": evaluation_text\n        }\n        \n        # Format a user-friendly evaluation message\n        evaluation_message = f\"\"\"# Hindi Conversation Evaluation\n\n## Overall Score: {\"⭐\" * evaluation_result[\"overall_score\"]} ({evaluation_result[\"overall_score\"]}/5)\n\n{evaluation_text}\n\nWould you like to try another scenario?\n\"\"\"\n        \n        # Update state with evaluation\n        new_state = state.copy()\n        new_state[\"evaluation\"] = evaluation_result\n        new_state[\"messages\"] = messages + [AIMessage(content=evaluation_message)]\n        new_state[\"current_stage\"] = \"done\"\n        \n        return new_state\n        \n    def router_agent(self, state: ConversationState) -> str:\n        \"\"\"Determine the next agent to handle the conversation\"\"\"\n        current_stage = state[\"current_stage\"]\n        return current_stage\n\ndef build_conversation_graph(generator_llm, hindi_model, hindi_tokenizer, evaluator_llm, rag_system):\n    \"\"\"Build the LangGraph conversation flow with separate models for different roles\"\"\"\n    # Create agents\n    agents = LangGraphAgents(generator_llm, hindi_model, hindi_tokenizer, evaluator_llm, rag_system)\n    \n    # Define the workflow graph\n    workflow = StateGraph(ConversationState)\n    \n    # Add nodes\n    workflow.add_node(\"scenario_generator\", agents.scenario_generator_agent)\n    workflow.add_node(\"language_tutor\", agents.language_tutor_agent)\n    workflow.add_node(\"evaluator\", agents.evaluator_agent)\n    \n    # Add edges\n    workflow.add_edge(\"scenario_generator\", \"language_tutor\")\n    \n    # Add conditional edges using a simpler router function\n    def router(state):\n        \"\"\"Simple router based on current_stage field\"\"\"\n        return state[\"current_stage\"]\n        \n    workflow.add_conditional_edges(\n        \"language_tutor\",\n        router,\n        {\n            \"language_tutor\": \"language_tutor\",\n            \"evaluator\": \"evaluator\",\n            \"done\": END\n        }\n    )\n    \n    # After evaluation, end the conversation\n    workflow.add_edge(\"evaluator\", END)\n    \n    # Set the entry point\n    workflow.set_entry_point(\"scenario_generator\")\n    \n    return workflow.compile()\n\nprint(\"LangGraph components defined!\")\n\n# Cell 7: Optional Speech Components\n# These functions are only used if ENABLE_SPEECH is True\n\ndef setup_speech_components():\n    \"\"\"Set up speech recognition and TTS components if enabled\"\"\"\n    if not ENABLE_SPEECH:\n        print(\"Speech features disabled. Set ENABLE_SPEECH = True to enable.\")\n        return None, None\n        \n    try:\n        import whisper\n        from gtts import gTTS\n        \n        # Load a small model for speech recognition\n        print(\"Loading Whisper base model for speech recognition...\")\n        speech_model = whisper.load_model(\"base\")\n        \n        print(\"Speech components initialized successfully.\")\n        return speech_model, True\n    except Exception as e:\n        print(f\"Error initializing speech components: {e}\")\n        return None, None\n\ndef transcribe_audio(speech_model, audio_path):\n    \"\"\"Transcribe audio using Whisper\"\"\"\n    if not speech_model or not ENABLE_SPEECH:\n        return \"Speech recognition is disabled.\"\n        \n    try:\n        result = speech_model.transcribe(audio_path, language=\"hi\")\n        return result[\"text\"]\n    except Exception as e:\n        return f\"Error transcribing audio: {e}\"\n\ndef text_to_speech(text, output_path=\"/kaggle/working/tutor_speak.mp3\"):\n    \"\"\"Convert text to speech using gTTS\"\"\"\n    if not ENABLE_SPEECH:\n        return None\n        \n    # Extract just Hindi text if it contains both Roman and Devanagari\n    lines = text.strip().split(\"\\n\")\n    if len(lines) >= 2 and not text.startswith(\"#\"):\n        # Use both lines for better speech synthesis\n        text_for_tts = f\"{lines[0]} {lines[1]}\"\n    \n    try:\n        from gtts import gTTS\n        tts = gTTS(text=text_for_tts if 'text_for_tts' in locals() else text, lang=\"hi\", slow=False)\n        tts.save(output_path)\n        return output_path\n    except Exception as e:\n        print(f\"Error generating speech: {e}\")\n        return None\n\nprint(\"Speech components defined (will be enabled if ENABLE_SPEECH=True)\")\n\n# Cell 8: Main Application Class\n# This is the core class that runs the system\n\nclass EnhancedHindiPracticeApp:\n    \"\"\"Main application class for the Hindi practice system with LangGraph integration\"\"\"\n    \n    def __init__(self):\n        self.hindi_model = None\n        self.hindi_tokenizer = None\n        self.generator_model = None\n        self.generator_tokenizer = None\n        self.generator_llm = None\n        self.evaluator_model = None\n        self.evaluator_tokenizer = None\n        self.evaluator_llm = None\n        self.rag_system = None\n        self.conversation_graph = None\n        self.state = None\n        self.model_loaded = False\n        self.speech_model = None\n        self.speech_enabled = False\n\n   \n    def initialize_system(self, progress=None):\n        \"\"\"Initialize all components of the system with progressive loading for the recommended models\"\"\"\n        try:\n            if progress:\n                progress(0, desc=\"Initializing Hindi Learning System...\")\n                \n            # Configure PyTorch to properly use GPU\n            if DEVICE == \"cuda\":\n                # Set higher memory limits for 16GB VRAM\n                os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:4096'\n                torch.cuda.empty_cache()\n                print(f\"GPU memory before loading: {torch.cuda.memory_allocated()/1024**3:.2f}GB / {torch.cuda.get_device_properties(0).total_memory/1024**3:.2f}GB\")\n            \n            # Initialize RAG system first\n            try:\n                if progress:\n                    progress(0.1, desc=\"Setting up RAG system...\")\n                self.rag_system = HindiLearningRAG(dummy_mode=False)  # Try with actual retrieval\n                \n                # Initialize RAG data\n                if progress:\n                    progress(0.2, desc=\"Setting up dialogue data...\")\n                if not os.path.exists(RAG_DATA_PATH):\n                    create_sample_dialogue_data()\n                \n                # Load or create vector store\n                if progress:\n                    progress(0.25, desc=\"Loading vector store...\")\n                self.rag_system.load_documents()\n                self.rag_system.create_vector_store()\n                        \n            except Exception as e:\n                print(f\"RAG initialization error: {str(e)}\")\n                if progress:\n                    progress(0.25, desc=\"Using fallback RAG system\")\n                self.rag_system = HindiLearningRAG(dummy_mode=True)\n            \n            # 1. Load Hindi conversation model\n            if progress:\n                progress(0.3, desc=\"Loading Hindi conversation model...\")\n            try:\n                # Clear memory before loading\n                clear_gpu_memory()\n                \n                # Use 4-bit quantization for efficient loading\n                quantization_config = BitsAndBytesConfig(\n                    load_in_4bit=True,\n                    bnb_4bit_compute_dtype=torch.float16,\n                    bnb_4bit_use_double_quant=True,\n                    bnb_4bit_quant_type=\"nf4\"\n                )\n                \n                self.hindi_model = AutoModelForCausalLM.from_pretrained(\n                    HINDI_MODEL_NAME,\n                    quantization_config=quantization_config,\n                    device_map=\"auto\",\n                    torch_dtype=torch.float16\n                )\n                \n                self.hindi_tokenizer = AutoTokenizer.from_pretrained(HINDI_MODEL_NAME)\n                if self.hindi_tokenizer.pad_token is None:\n                    self.hindi_tokenizer.pad_token = self.hindi_tokenizer.eos_token\n                    \n                print(f\"Hindi model loaded. GPU memory: {torch.cuda.memory_allocated()/1024**3:.2f}GB\")\n                \n            except Exception as e:\n                error_msg = f\"Error loading Hindi model: {str(e)}\"\n                print(error_msg)\n                if progress:\n                    progress(0.4, desc=error_msg)\n                return error_msg\n            \n            # 2. Load generator model (separate model)\n            if progress:\n                progress(0.5, desc=\"Loading scenario generator model...\")\n            try:\n                # Use different quantization for generator model\n                generator_quantization_config = BitsAndBytesConfig(\n                    load_in_4bit=True,\n                    bnb_4bit_compute_dtype=torch.float16,\n                    bnb_4bit_use_double_quant=True,\n                    bnb_4bit_quant_type=\"nf4\"\n                )\n                \n                # Load a different model for generation tasks\n                self.generator_model = AutoModelForCausalLM.from_pretrained(\n                    \"meta-llama/Llama-3.2-3B-Instruct\",  # Using Llama-3.2 base as fallback\n                    quantization_config=generator_quantization_config,\n                    device_map=\"auto\",\n                    torch_dtype=torch.float16\n                )\n                \n                self.generator_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n                if self.generator_tokenizer.pad_token is None:\n                    self.generator_tokenizer.pad_token = self.generator_tokenizer.eos_token\n                \n                # Create wrapper with corrected class\n                self.generator_llm = TemplateModelWrapper(\n                    self.generator_model, \n                    self.generator_tokenizer,\n                    model_name=\"scenario-generator\"\n                )\n                \n                print(f\"Generator model loaded. GPU memory: {torch.cuda.memory_allocated()/1024**3:.2f}GB\")\n            \n            except Exception as e:\n                print(f\"Error loading generator model: {str(e)}\")\n                # Fallback to use the Hindi model for generation if needed\n                if progress:\n                    progress(0.6, desc=\"Using Hindi model for generation\")\n                self.generator_model = self.hindi_model\n                self.generator_tokenizer = self.hindi_tokenizer\n                self.generator_llm = TemplateModelWrapper(\n                    self.generator_model, \n                    self.generator_tokenizer,\n                    model_name=\"hindi-as-generator\"\n                )\n            \n            # 3. Load evaluator model (separate model)\n            if progress:\n                progress(0.7, desc=\"Loading evaluator model...\")\n            try:\n                # Use 4-bit quantization for evaluator\n                evaluator_quantization_config = BitsAndBytesConfig(\n                    load_in_4bit=True,\n                    bnb_4bit_compute_dtype=torch.float16,\n                    bnb_4bit_use_double_quant=True,\n                    bnb_4bit_quant_type=\"nf4\"\n                )\n                \n                self.evaluator_model = AutoModelForCausalLM.from_pretrained(\n                    EVALUATOR_MODEL_NAME,\n                    quantization_config=evaluator_quantization_config,\n                    device_map=\"auto\",\n                    torch_dtype=torch.float16\n                )\n            \n                self.evaluator_tokenizer = AutoTokenizer.from_pretrained(EVALUATOR_MODEL_NAME)\n                if self.evaluator_tokenizer.pad_token is None:\n                    self.evaluator_tokenizer.pad_token = self.evaluator_tokenizer.eos_token\n                \n                # Create evaluator with corrected class\n                self.evaluator_llm = TemplateModelWrapper(\n                    self.evaluator_model,\n                    self.evaluator_tokenizer,\n                    model_name=\"performance-evaluator\"\n                )\n                \n                print(f\"Evaluator model loaded. GPU memory: {torch.cuda.memory_allocated()/1024**3:.2f}GB\")\n                \n            except Exception as e:\n                print(f\"Error loading evaluator model: {str(e)}\")\n                # Fallback to use the Hindi model for evaluation\n                if progress:\n                    progress(0.8, desc=\"Using Hindi model for evaluation\")\n                self.evaluator_model = self.hindi_model\n                self.evaluator_tokenizer = self.hindi_tokenizer\n                self.evaluator_llm = TemplateModelWrapper(\n                    self.evaluator_model,\n                    self.evaluator_tokenizer,\n                    model_name=\"hindi-as-evaluator\"\n                )\n        \n            # Build the conversation graph with all separate models\n            if progress:\n                progress(0.9, desc=\"Building conversation graph...\")\n            try:\n                self.conversation_graph = build_conversation_graph(\n                    self.generator_llm,\n                    self.hindi_model,\n                    self.hindi_tokenizer,\n                    self.evaluator_llm,\n                    self.rag_system\n                )\n                \n                print(f\"Conversation graph built. Final GPU memory: {torch.cuda.memory_allocated()/1024**3:.2f}GB\")\n                \n            except Exception as e:\n                error_msg = f\"Error building conversation graph: {str(e)}\"\n                print(error_msg)\n                if progress:\n                    progress(0.95, desc=error_msg)\n                return error_msg\n            \n            self.model_loaded = True\n            if progress:\n                progress(1.0, desc=\"System initialized successfully!\")\n                \n            return f\"Hindi Learning System initialized! GPU memory used: {torch.cuda.memory_allocated()/1024**3:.2f}GB\"\n        \n        except Exception as e:\n            error_msg = f\"System initialization failed: {str(e)}\"\n            print(error_msg)\n            if progress:\n                progress(1.0, desc=error_msg)\n            return error_msg\n\n\n    \n    def start_new_scenario(self, proficiency_level=\"beginner\"):\n        \"\"\"Start a new conversation scenario with improved error handling\"\"\"\n        if not self.model_loaded:\n            return [], \"Model not loaded. Please initialize the system first.\", \"\"\n            \n        try:\n            # Initialize state with a human message requesting a scenario\n            self.state = {\n                \"messages\": [HumanMessage(content=f\"Create a new Hindi conversation scenario for {proficiency_level} level\")],\n                \"scenario\": None,\n                \"evaluation\": None,\n                \"dialogue_history\": [],\n                \"current_stage\": \"scenario_generator\",\n                \"user_input\": None,\n                \"user_used_hindi\": False,\n                \"proficiency_level\": proficiency_level\n            }\n            \n            # Use a fallback scenario in case of any errors\n            fallback_scenario = get_fallback_scenario(proficiency_level)\n            \n            try:\n                # Try to generate scenario using LangGraph\n                print(\"Generating new scenario using conversation graph...\")\n                self.state = self.conversation_graph.invoke(self.state)\n                \n                # Check if scenario was generated successfully\n                if not self.state.get(\"scenario\"):\n                    print(\"No scenario generated by graph, using fallback...\")\n                    self.state[\"scenario\"] = fallback_scenario\n            except Exception as e:\n                print(f\"Error generating scenario through conversation graph: {e}\")\n                # Handle error by using fallback scenario\n                self.state[\"scenario\"] = fallback_scenario\n                \n                # Create appropriate messages\n                system_message = SystemMessage(content=f\"A default scenario has been loaded: {fallback_scenario['scenario_title']}\")\n                \n                # Use first line from fallback scenario\n                first_line_roman = fallback_scenario[\"first_line\"].get(\"roman\", \"Namaste!\")\n                first_line_devanagari = fallback_scenario[\"first_line\"].get(\"devanagari\", \"नमस्ते!\")\n                first_message = AIMessage(content=f\"{first_line_roman}\\n{first_line_devanagari}\")\n                \n                self.state[\"messages\"] = self.state[\"messages\"] + [system_message, first_message]\n                self.state[\"dialogue_history\"] = [{\n                    \"role\": \"assistant\",\n                    \"content\": f\"{first_line_roman}\\n{first_line_devanagari}\"\n                }]\n                self.state[\"current_stage\"] = \"language_tutor\"\n            \n            # Extract scenario details for UI\n            scenario = self.state[\"scenario\"]\n            scenario_details = f\"# {scenario['scenario_title']}\\n\\n\"\n            scenario_details += f\"{scenario['scenario_description']}\\n\\n\"\n            scenario_details += f\"**Your Role**: {scenario['user_role']}\\n\"\n            scenario_details += f\"**AI's Role**: {scenario['character_role']}\\n\\n\"\n            scenario_details += \"**Your Goals**:\\n\"\n            for goal in scenario['goals']:\n                scenario_details += f\"- {goal}\\n\"\n                \n            # Format vocabulary list\n            vocabulary = \"### Useful Hindi Vocabulary:\\n\"\n            for vocab in scenario['key_vocabulary']:\n                if isinstance(vocab, dict):\n                    roman = vocab.get('roman', '')\n                    devanagari = vocab.get('devanagari', '')\n                    meaning = vocab.get('meaning', '')\n                    vocabulary += f\"- {roman} ({devanagari}): {meaning}\\n\"\n                else:\n                    vocabulary += f\"- {vocab}\\n\"\n                    \n            # Format the conversation for Gradio chatbot\n            formatted_messages = []\n            for msg in self.state[\"messages\"]:\n                if isinstance(msg, SystemMessage):\n                    # Skip system messages in the UI\n                    continue\n                elif isinstance(msg, AIMessage):\n                    content = msg.content\n                    # Format AI messages for better display\n                    lines = content.strip().split(\"\\n\")\n                    if len(lines) >= 2 and not content.startswith(\"#\"):  # Not an evaluation message\n                        roman = lines[0]\n                        devanagari = lines[1]\n                        content = f\"🗣️ {roman}\\n📝 {devanagari}\"\n                    formatted_messages.append((\"\", content))\n                elif isinstance(msg, HumanMessage):\n                    formatted_messages.append((f\"👤 {msg.content}\", \"\"))\n            \n            return formatted_messages, scenario_details, vocabulary\n            \n        except Exception as e:\n            error_msg = f\"Error starting new scenario: {str(e)}\"\n            print(error_msg)\n            return [], error_msg, \"\"\n            \n    def send_message(self, user_input, history):\n        \"\"\"Process a user message and continue the conversation\"\"\"\n        if not self.model_loaded or not self.state:\n            return history + [(f\"👤 {user_input}\", \"System not initialized. Please start a new scenario first.\")]\n                \n        if not user_input:\n            return history\n                \n        try:\n            # Update state with user input\n            self.state[\"user_input\"] = user_input\n            \n            # Process through the conversation graph\n            try:\n                self.state = self.conversation_graph.invoke(self.state)\n            except Exception as e:\n                error_msg = f\"Error processing message: {str(e)}\"\n                print(error_msg)\n                return history + [(f\"👤 {user_input}\", f\"Error: {error_msg}\")]\n                \n            # Format the latest AI message\n            latest_ai_message = None\n            for msg in reversed(self.state[\"messages\"]):\n                if isinstance(msg, AIMessage):\n                    latest_ai_message = msg\n                    break\n                    \n            if latest_ai_message:\n                content = latest_ai_message.content\n                # Check if this is a regular message or evaluation\n                if not content.startswith(\"#\"):  # Not an evaluation\n                    lines = content.strip().split(\"\\n\")\n                    if len(lines) >= 2:\n                        roman = lines[0]\n                        devanagari = lines[1]\n                        formatted_content = f\"🗣️ {roman}\\n📝 {devanagari}\"\n                    else:\n                        formatted_content = content\n                else:\n                    # This is an evaluation message, keep formatting\n                    formatted_content = content\n                    \n                updated_history = history + [(f\"👤 {user_input}\", formatted_content)]\n            else:\n                updated_history = history + [(f\"👤 {user_input}\", \"No response generated\")]\n                \n            return updated_history\n            \n        except Exception as e:\n            error_msg = f\"Error in send_message: {str(e)}\"\n            print(error_msg)\n            return history + [(f\"👤 {user_input}\", f\"Error: {error_msg}\")]\n            \n    def get_speech_from_text(self, text=\"\"):\n        \"\"\"Generate speech from text using gTTS\"\"\"\n        if not self.speech_enabled:\n            return None\n                \n        if not text:\n            # Get the last AI message\n            for msg in reversed(self.state[\"messages\"]):\n                if isinstance(msg, AIMessage):\n                    text = msg.content\n                    break\n                        \n        # Generate speech\n        return text_to_speech(text)\n                \n    def transcribe_audio(self, audio_path):\n        \"\"\"Transcribe audio to text using Whisper\"\"\"\n        if not self.speech_enabled or not self.speech_model:\n            return \"Speech recognition is disabled\"\n                \n        return transcribe_audio(self.speech_model, audio_path)# Cell 8: Main Application Class\n# This is the core class that runs the system\n\n            \n\n\n\n\n\n\nprint(\"Main application class defined!\")\n\n# Cell 9: Gradio Interface\n# This creates the user interface for the system\n\ndef create_enhanced_gradio_interface():\n    \"\"\"Create the Gradio interface for the enhanced Hindi practice system with better error handling\"\"\"\n    app = EnhancedHindiPracticeApp()\n    \n    with gr.Blocks(title=\"Enhanced Hindi Conversation Practice\", theme=gr.themes.Soft()) as interface:\n        gr.Markdown(\"# 🇮🇳 Enhanced Hindi Conversation Practice\")\n        gr.Markdown(\"Practice Hindi in dynamic roleplay scenarios with AI tutoring and feedback\")\n        \n        with gr.Row():\n            with gr.Column(scale=1):\n                # Add progress indicator and status message\n                status_msg = gr.Markdown(\"System status: Not initialized\")\n                init_progress = gr.Textbox(\n                    label=\"Initialization Progress\",\n                    value=\"Click 'Initialize System' to begin\",\n                    interactive=False\n                )\n                init_button = gr.Button(\"Initialize System\", variant=\"primary\")\n                \n                with gr.Accordion(\"New Scenario\", open=True):\n                    proficiency_selector = gr.Radio(\n                        choices=[\"beginner\", \"intermediate\", \"advanced\"],\n                        label=\"Choose your proficiency level\",\n                        value=\"beginner\"\n                    )\n                    start_scenario_button = gr.Button(\"Start New Scenario\", variant=\"secondary\")\n                \n                scenario_description = gr.Markdown(\"Initialize the system and start a scenario to begin practicing.\")\n                vocabulary_section = gr.Markdown(\"Vocabulary will appear here.\")\n                \n                with gr.Accordion(\"About This Enhanced App\", open=False):\n                    gr.Markdown(\"\"\"\n                    This enhanced Hindi practice app uses LangGraph to create:\n                    \n                    1. **Dynamic Scenarios**: Each practice session features a unique scenario tailored to your proficiency level\n                    2. **Natural Conversation**: Practice with a Hindi-speaking AI tutor in realistic situations\n                    3. **Performance Evaluation**: Get detailed feedback on your conversation skills\n                    \n                    How to use:\n                    1. Click \"Initialize System\" to set up the AI\n                    2. Select your proficiency level\n                    3. Start a new scenario\n                    4. Practice the conversation, trying to accomplish the goals\n                    5. Get feedback on your performance\n                    \n                    Try to use Hindi words and phrases as much as possible!\n                    \"\"\")\n                \n                # Add debug information section\n                with gr.Accordion(\"Debug Information\", open=False):\n                    debug_info = gr.Textbox(\n                        label=\"Debug Output\",\n                        value=\"Debug information will appear here\",\n                        interactive=False\n                    )\n                    debug_refresh = gr.Button(\"Refresh Debug Info\")\n            \n            with gr.Column(scale=2):\n                chatbot = gr.Chatbot(\n                    height=500,\n                    show_label=False,\n                    elem_id=\"hindi_langraph_chatbot\"\n                )\n                \n                with gr.Row():\n                    user_input = gr.Textbox(\n                        placeholder=\"Type your response here...\",\n                        show_label=False,\n                        scale=8\n                    )\n                    send_button = gr.Button(\"Send\", scale=1)\n                    mic_button = gr.Button(\"🎤\", size=\"sm\", scale=1, visible=ENABLE_SPEECH)\n                \n                with gr.Row(visible=ENABLE_SPEECH):\n                    speak_button = gr.Button(\"🔊 Hear Tutor\")\n                    tutor_audio = gr.Audio(label=\"\", autoplay=True)\n            \n            with gr.Column(visible=False) as audio_popup:\n                gr.Markdown(\"### Speak Hindi\")\n                audio_input = gr.Audio(sources=[\"microphone\"], type=\"filepath\", label=\"Record your voice\")\n                submit_audio = gr.Button(\"Submit Recording\")\n        \n        # Create progress updater function\n        def update_progress(progress_value, desc=\"\"):\n            percentage = int(progress_value * 100)\n            return f\"{desc} ({percentage}%)\"\n        \n        # Modified initialization function\n        def init_system():\n            # Reset status\n            yield update_progress(0, \"Starting initialization...\"), \"System status: Initializing...\"\n            \n            try:\n                # Call the initialize method with progress updates\n                result = app.initialize_system(\n                    progress=lambda value, desc: gr.update(value=update_progress(value, desc))\n                )\n                \n                # Check if initialization succeeded\n                if app.model_loaded:\n                    yield update_progress(1, \"Initialization complete!\"), \"System status: Ready\"\n                else:\n                    yield update_progress(1, f\"Initialization failed: {result}\"), f\"System status: Error - {result}\"\n            except Exception as e:\n                error_msg = f\"Error during initialization: {str(e)}\"\n                yield update_progress(1, f\"Error: {error_msg}\"), f\"System status: Error - {error_msg}\"\n        \n        # Debug info refresh function\n        def refresh_debug_info():\n            if not hasattr(app, 'model_loaded'):\n                return \"App not initialized yet\"\n                \n            debug_text = f\"Model loaded: {app.model_loaded}\\n\"\n            if hasattr(app, 'hindi_model') and app.hindi_model:\n                debug_text += f\"Hindi model loaded: Yes\\n\"\n            else:\n                debug_text += f\"Hindi model loaded: No\\n\"\n                \n            if hasattr(app, 'generator_model') and app.generator_model:\n                debug_text += f\"Generator model loaded: Yes\\n\"\n            else:\n                debug_text += f\"Generator model loaded: No\\n\"\n                \n            if hasattr(app, 'evaluator_model') and app.evaluator_model:\n                debug_text += f\"Evaluator model loaded: Yes\\n\"\n            else:\n                debug_text += f\"Evaluator model loaded: No\\n\"\n                \n            if hasattr(app, 'conversation_graph') and app.conversation_graph:\n                debug_text += f\"Conversation graph built: Yes\\n\"\n            else:\n                debug_text += f\"Conversation graph built: No\\n\"\n                \n            if hasattr(app, 'state') and app.state:\n                debug_text += f\"Current state: {app.state.get('current_stage', 'Not set')}\\n\"\n                \n            # Add GPU memory info\n            if torch.cuda.is_available():\n                debug_text += f\"GPU memory usage: {torch.cuda.memory_allocated()/1024**3:.2f}GB / {torch.cuda.get_device_properties(0).total_memory/1024**3:.2f}GB\\n\"\n                \n            return debug_text\n        \n        # Modified check_start_scenario function with detailed error reporting\n        def check_start_scenario(proficiency):\n            try:\n                if not app.model_loaded:\n                    return [(None, \"Please initialize the system first\")], \"System not initialized\", \"Please initialize the system first\", \"Error: System not initialized\"\n                \n                print(f\"Starting new scenario with proficiency level: {proficiency}\")\n                \n                try:\n                    result = app.start_new_scenario(proficiency)\n                    print(f\"Scenario generation completed\")\n                    return result[0], result[1], result[2], refresh_debug_info()\n                except Exception as e:\n                    import traceback\n                    error_details = traceback.format_exc()\n                    print(f\"Error in start_new_scenario: {str(e)}\")\n                    print(f\"Error details: {error_details}\")\n                    \n                    error_msg = f\"Error generating scenario: {str(e)}\"\n                    return [(None, error_msg)], error_msg, f\"Error details: {str(e)}\", error_details\n                    \n            except Exception as e:\n                import traceback\n                error_details = traceback.format_exc()\n                print(f\"Exception in check_start_scenario: {str(e)}\")\n                print(f\"Error details: {error_details}\")\n                return [(None, f\"System error: {str(e)}\")], f\"Error: {str(e)}\", \"Please try again after restarting\", error_details\n        \n        # Set up event handlers with updated progress handling\n        init_button.click(\n            init_system,\n            outputs=[init_progress, status_msg]\n        )\n        \n        # Connect debug refresh button\n        debug_refresh.click(\n            refresh_debug_info,\n            outputs=[debug_info]\n        )\n        \n        # Connect scenario button with more outputs for debugging\n        start_scenario_button.click(\n            check_start_scenario,\n            inputs=[proficiency_selector],\n            outputs=[chatbot, scenario_description, vocabulary_section, debug_info]\n        )\n        \n        user_input.submit(\n            app.send_message,\n            inputs=[user_input, chatbot],\n            outputs=[chatbot]\n        ).then(\n            lambda: \"\",  # Clear input after sending\n            outputs=[user_input]\n        )\n        \n        send_button.click(\n            app.send_message,\n            inputs=[user_input, chatbot],\n            outputs=[chatbot]\n        ).then(\n            lambda: \"\",  # Clear input after sending\n            outputs=[user_input]\n        )\n        \n        if ENABLE_SPEECH:\n            mic_button.click(\n                lambda: gr.update(visible=True),\n                outputs=[audio_popup]\n            )\n            \n            submit_audio.click(\n                app.transcribe_audio,\n                inputs=[audio_input],\n                outputs=[user_input]\n            ).then(\n                lambda: gr.update(visible=False),\n                outputs=[audio_popup]\n            )\n            \n            speak_button.click(\n                app.get_speech_from_text,\n                outputs=[tutor_audio]\n            )\n        \n    return interface, app\nprint(\"Gradio interface defined!\")\n\n# Cell 10: Data Preparation Helper (optional)\n# This cell helps prepare sample data if you don't have the processed_hindi_dialogues.json file\n\ndef create_sample_dialogue_data():\n    \"\"\"Create a sample dataset if the original data is not available\"\"\"\n    sample_data = []\n    \n    # Sample 1: Restaurant scenario\n    restaurant_sample = {\n        \"page_content\": \"A conversation in a restaurant between a waiter and customer.\",\n        \"metadata\": {\n            \"scene_description\": \"A busy restaurant in Delhi during lunchtime.\",\n            \"context_tags\": [\"restaurant\", \"food\", \"dining\"],\n            \"emotion_tags\": [\"neutral\", \"polite\"],\n            \"roman_dialogue\": \"Waiter: Namaste ji, kya khaayenge aap? Customer: Menu dikha dijiye. Waiter: Ji, yeh lijiye menu.\",\n            \"devanagari_dialogue\": \"वेटर: नमस्ते जी, क्या खाएंगे आप? कस्टमर: मेनू दिखा दीजिए। वेटर: जी, यह लीजिए मेनू।\",\n            \"dialogue_turns\": [\n                {\n                    \"speaker\": \"Waiter\",\n                    \"text_roman\": \"Namaste ji, kya khaayenge aap?\",\n                    \"text_devanagari\": \"नमस्ते जी, क्या खाएंगे आप?\"\n                },\n                {\n                    \"speaker\": \"Customer\",\n                    \"text_roman\": \"Menu dikha dijiye.\",\n                    \"text_devanagari\": \"मेनू दिखा दीजिए।\"\n                },\n                {\n                    \"speaker\": \"Waiter\",\n                    \"text_roman\": \"Ji, yeh lijiye menu.\",\n                    \"text_devanagari\": \"जी, यह लीजिए मेनू।\"\n                }\n            ]\n        }\n    }\n    \n    # Sample 2: Market scenario\n    market_sample = {\n        \"page_content\": \"A conversation in a market between a shopkeeper and customer.\",\n        \"metadata\": {\n            \"scene_description\": \"A busy vegetable market in Mumbai.\",\n            \"context_tags\": [\"market\", \"shopping\", \"bazaar\"],\n            \"emotion_tags\": [\"neutral\", \"negotiating\"],\n            \"roman_dialogue\": \"Shopkeeper: Aaiye ji, kya chahiye? Customer: Tamatar kitne ka hai? Shopkeeper: Sau rupaye kilo, ekdum taza hai.\",\n            \"devanagari_dialogue\": \"दुकानदार: आइए जी, क्या चाहिए? ग्राहक: टमाटर कितने का है? दुकानदार: सौ रुपये किलो, एकदम ताज़ा है।\",\n            \"dialogue_turns\": [\n                {\n                    \"speaker\": \"Shopkeeper\",\n                    \"text_roman\": \"Aaiye ji, kya chahiye?\",\n                    \"text_devanagari\": \"आइए जी, क्या चाहिए?\"\n                },\n                {\n                    \"speaker\": \"Customer\",\n                    \"text_roman\": \"Tamatar kitne ka hai?\",\n                    \"text_devanagari\": \"टमाटर कितने का है?\"\n                },\n                {\n                    \"speaker\": \"Shopkeeper\",\n                    \"text_roman\": \"Sau rupaye kilo, ekdum taza hai.\",\n                    \"text_devanagari\": \"सौ रुपये किलो, एकदम ताज़ा है।\"\n                }\n            ]\n        }\n    }\n    \n    # Sample 3: Hotel scenario\n    hotel_sample = {\n        \"page_content\": \"A conversation at a hotel reception.\",\n        \"metadata\": {\n            \"scene_description\": \"Check-in at a mid-range hotel in Jaipur.\",\n            \"context_tags\": [\"hotel\", \"travel\", \"accommodation\"],\n            \"emotion_tags\": [\"formal\", \"polite\"],\n            \"roman_dialogue\": \"Receptionist: Namaste ji, swagat hai aapka. Guest: Meri booking hai, Singh ke naam se. Receptionist: Ji, ek minute dekhta hoon.\",\n            \"devanagari_dialogue\": \"रिसेप्शनिस्ट: नमस्ते जी, स्वागत है आपका। अतिथि: मेरी बुकिंग है, सिंह के नाम से। रिसेप्शनिस्ट: जी, एक मिनट देखता हूँ।\",\n            \"dialogue_turns\": [\n                {\n                    \"speaker\": \"Receptionist\",\n                    \"text_roman\": \"Namaste ji, swagat hai aapka.\",\n                    \"text_devanagari\": \"नमस्ते जी, स्वागत है आपका।\"\n                },\n                {\n                    \"speaker\": \"Guest\",\n                    \"text_roman\": \"Meri booking hai, Singh ke naam se.\",\n                    \"text_devanagari\": \"मेरी बुकिंग है, सिंह के नाम से।\"\n                },\n                {\n                    \"speaker\": \"Receptionist\",\n                    \"text_roman\": \"Ji, ek minute dekhta hoon.\",\n                    \"text_devanagari\": \"जी, एक मिनट देखता हूँ।\"\n                }\n            ]\n        }\n    }\n    \n    sample_data.extend([restaurant_sample, market_sample, hotel_sample])\n    \n    # Save to file\n    if not os.path.exists(os.path.dirname(RAG_DATA_PATH)):\n        os.makedirs(os.path.dirname(RAG_DATA_PATH))\n        \n    with open(RAG_DATA_PATH, 'w', encoding='utf-8') as f:\n        json.dump(sample_data, f, ensure_ascii=False, indent=2)\n    \n    print(f\"Sample dialogue data created at {RAG_DATA_PATH}\")\n    return sample_data\n\nprint(\"Data preparation helper defined!\")\n\n# Cell 11: Launch the application\n# This cell starts the app and provides a public link\n\ndef main():\n    \"\"\"Main function to launch the application with memory optimizations\"\"\"\n    # Set up aggressive memory management\n    import os\n    import gc\n    import torch\n    \n    # Configure PyTorch for better memory management\n    torch.cuda.empty_cache()\n    if torch.cuda.is_available():\n        # Set to release memory aggressively\n        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n        \n    # First, check if we need to create sample data\n    if not os.path.exists(RAG_DATA_PATH):\n        print(\"Creating sample dialogue data...\")\n        create_sample_dialogue_data()\n    \n    print(\"Initializing Enhanced Hindi Language Learning System with LangGraph...\")\n    interface, app = create_enhanced_gradio_interface()\n    \n    # # Launch with share=True to create a public link\n    # # Lower the max_threads for less memory usage\n    # interface.launch(\n    #     share=True,\n    #     server_name=\"0.0.0.0\",\n    #     server_port=7861,\n    #     max_threads=20  # Reduce thread count to save memory\n    # )\n\n# # Run the application\n# if __name__ == \"__main__\":\n#     main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T06:10:56.435151Z","iopub.execute_input":"2025-04-24T06:10:56.435542Z","iopub.status.idle":"2025-04-24T06:13:27.896985Z","shell.execute_reply.started":"2025-04-24T06:10:56.435515Z","shell.execute_reply":"2025-04-24T06:13:27.896304Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# WITH SPEECH FEATURES","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def manual_initialize_fixed():\n    app = EnhancedHindiPracticeApp()\n    \n    print(\"Starting fixed manual initialization with optimized models...\")\n    clear_gpu_memory()\n    \n    # Define the Whisper model size - this was missing\n    WHISPER_MODEL_SIZE = \"base\"  # Options: tiny, base, small, medium, large\n    \n    # Initialize RAG with proper file path\n    try:\n        print(\"Initializing RAG system with preprocessed data...\")\n        app.rag_system = HindiLearningRAG(dummy_mode=False)\n        \n        # Try to load or create the vector store\n        if not os.path.exists(RAG_DATA_PATH):\n            print(f\"Creating sample dialogue data at {RAG_DATA_PATH}...\")\n            create_sample_dialogue_data()\n        \n        app.rag_system.load_documents()\n        if not app.rag_system.vector_store:\n            app.rag_system.create_vector_store()\n            \n        print(\"RAG system initialized with vector store\")\n    except Exception as e:\n        print(f\"RAG initialization error: {str(e)}\")\n        print(\"Falling back to dummy RAG mode\")\n        app.rag_system = HindiLearningRAG(dummy_mode=True)\n    \n    # Load Hindi model with OPTIMIZED parameters for better generation\n    print(\"Loading Hindi model with enhanced parameters...\")\n    if USE_4BIT:\n        # Use improved quantization config with higher compute precision\n        quantization_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_compute_dtype=torch.float16,  # Use float16 for better precision\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\"\n        )\n        \n        app.hindi_model = AutoModelForCausalLM.from_pretrained(\n            HINDI_MODEL_NAME,\n            quantization_config=quantization_config,\n            device_map=\"auto\",\n            torch_dtype=torch.float16,\n            # Add additional loading parameters for higher quality\n            low_cpu_mem_usage=True\n        )\n    else:\n        # Fallback to 8-bit precision with optimized parameters\n        app.hindi_model = AutoModelForCausalLM.from_pretrained(\n            HINDI_MODEL_NAME,\n            load_in_8bit=True,\n            device_map=\"auto\",\n            torch_dtype=torch.float16\n        )\n    \n    app.hindi_tokenizer = AutoTokenizer.from_pretrained(HINDI_MODEL_NAME)\n    if app.hindi_tokenizer.pad_token is None:\n        app.hindi_tokenizer.pad_token = app.hindi_tokenizer.eos_token\n    \n    print(f\"Hindi model loaded. GPU memory: {torch.cuda.memory_allocated()/1024**3:.2f}GB\")\n    \n    # Use Hindi model for generator and evaluator\n    print(\"Configuring generator and evaluator...\")\n    app.generator_model = app.hindi_model\n    app.generator_tokenizer = app.hindi_tokenizer\n    app.evaluator_model = app.hindi_model\n    app.evaluator_tokenizer = app.hindi_tokenizer\n    \n    # Create proper wrapper with enhanced generation parameters\n    app.generator_llm = TemplateModelWrapper(\n        app.generator_model,\n        app.generator_tokenizer,\n        model_name=\"generator\"\n    )\n    \n    app.evaluator_llm = TemplateModelWrapper(\n        app.evaluator_model,\n        app.evaluator_tokenizer,\n        model_name=\"evaluator\"\n    )\n    \n    # Enable speech features\n    print(\"Setting up speech recognition capabilities...\")\n    try:\n        # Initialize speech recognizer\n        import whisper\n        app.speech_model = whisper.load_model(WHISPER_MODEL_SIZE)\n        app.speech_enabled = True\n        print(f\"Speech recognition enabled with Whisper {WHISPER_MODEL_SIZE} model\")\n    except Exception as e:\n        print(f\"Error initializing speech model: {e}\")\n        app.speech_enabled = False\n    \n    # Build the conversation graph (fix for the message validation)\n    print(\"Building fixed conversation graph with enhanced generation...\")\n    \n    # Create a custom version of the conversation flow that doesn't rely on complex state\n    class FixedLangGraphAgents:\n        def __init__(self, hindi_model, hindi_tokenizer, rag_system):\n            self.hindi_model = hindi_model\n            self.hindi_tokenizer = hindi_tokenizer\n            self.rag_system = rag_system\n            \n        # Replace the simple scenario generation with this more dynamic version\n        def generate_scenario(self, proficiency_level):\n            \"\"\"Generate a more dynamic scenario with higher temperature for creativity\"\"\"\n            print(f\"Generating dynamic scenario for {proficiency_level}...\")\n            \n            # Try to generate a creative scenario using the Hindi model\n            system_prompt = f\"\"\"Create a realistic Hindi conversation scenario for a {proficiency_level} language learner.\n            \n        Include:\n        1. scenario_type: Choose from restaurant, market, transportation, hotel, zoo, temple, office, school, etc.\n        2. scenario_title: A descriptive title\n        3. scenario_description: Brief setting description (2-3 sentences)\n        4. character_role: Role the AI plays (waiter, shopkeeper, etc.)\n        5. user_role: Role the learner plays (customer, traveler, etc.)\n        6. goals: 3-5 conversation goals for the learner\n        7. key_vocabulary: 5-8 Hindi words/phrases with Roman and Devanagari scripts\n        8. first_line: An opening dialogue line in both Roman and Devanagari\n\n    Format as JSON exactly like this:\n    {{\n    \"scenario_type\": \"market\",\n    \"scenario_title\": \"Bargaining at a Street Market\", \n    \"scenario_description\": \"You are shopping at a busy street market in Delhi...\",\n    \"character_role\": \"shopkeeper\",\n    \"user_role\": \"customer\",\n    \"goals\": [\"Ask about an item\", \"Negotiate the price\", \"Make a purchase\"],\n    \"key_vocabulary\": [\n        {{\"roman\": \"kitna\", \"devanagari\": \"कितना\", \"meaning\": \"how much\"}},\n        {{\"roman\": \"sasta\", \"devanagari\": \"सस्ता\", \"meaning\": \"cheap\"}}\n    ],\n    \"first_line\": {{\n        \"roman\": \"Kya chahiye aapko?\",\n        \"devanagari\": \"क्या चाहिए आपको?\"\n    }}\n    }}\"\"\"\n        \n            messages_formatted = [\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": f\"Create a unique {proficiency_level} level Hindi conversation scenario that's different from typical restaurant scenarios. Make it creative, educational and engaging. Include interesting and authentic cultural elements.\"}\n            ]\n            \n            try:\n                # Generate with higher temperature for more creativity\n                input_ids = self.hindi_tokenizer.apply_chat_template(\n                    messages_formatted,\n                    add_generation_prompt=True,\n                    return_tensors=\"pt\"\n                ).to(self.hindi_model.device)\n                \n                outputs = self.hindi_model.generate(\n                    input_ids,\n                    max_new_tokens=1000,  # Longer for better JSON generation\n                    do_sample=True,\n                    temperature=0.8,  # Higher for more creativity\n                    top_p=0.92,  # Slightly higher top_p\n                    repetition_penalty=1.15,\n                    eos_token_id=self.hindi_tokenizer.eos_token_id,\n                )\n                \n                generated_text = self.hindi_tokenizer.decode(\n                    outputs[0][input_ids.shape[-1]:], \n                    skip_special_tokens=True\n                )\n            \n                # Try to extract JSON\n                import re\n                import json\n                \n                # Find JSON-like structure\n                json_pattern = r'```json\\s*([\\s\\S]*?)\\s*```|(\\{[\\s\\S]*\\})'\n                match = re.search(json_pattern, generated_text)\n                \n                if match:\n                    json_str = match.group(1) or match.group(2)\n                    scenario_result = json.loads(json_str)\n                    \n                    # Add proficiency level for reference\n                    scenario_result[\"proficiency_level\"] = proficiency_level\n                    \n                    # Validate required fields\n                    required_fields = [\n                        \"scenario_type\", \"scenario_title\", \"scenario_description\", \n                        \"character_role\", \"user_role\", \"goals\", \"key_vocabulary\", \"first_line\"\n                    ]\n                    \n                    for field in required_fields:\n                        if field not in scenario_result:\n                            print(f\"Missing field {field} in generated scenario\")\n                            raise ValueError(f\"Missing field: {field}\")\n                            \n                    return scenario_result\n                else:\n                    print(\"Failed to extract JSON from generated scenario\")\n                    raise ValueError(\"Could not extract JSON from response\")\n                    \n            except Exception as e:\n                print(f\"Error generating dynamic scenario: {e}\")\n                print(\"Falling back to predefined scenario\")\n                # Use fallback with some variations\n                fallback = get_fallback_scenario(proficiency_level)\n                fallback[\"proficiency_level\"] = proficiency_level\n                return fallback\n            \n        def process_message(self, user_input, scenario, history=None):\n            \"\"\"Process user input with enhanced response generation\"\"\"\n            if history is None:\n                history = []\n                \n            print(f\"Processing message: {user_input}\")\n            \n            # Create system prompt for Hindi model\n            character_role = scenario[\"character_role\"]\n            scenario_type = scenario[\"scenario_type\"]\n            \n            # Check if user is using Hindi\n            user_used_hindi = is_hindi(user_input)\n            \n            system_prompt = f\"\"\"\nYou are a Hindi language tutor roleplaying as a {character_role} in a {scenario_type} scenario. \n    \nIMPORTANT:\n1. You MUST respond in this exact format:\n   - First line: Clear, natural Hindi in Roman script (1-2 sentences)\n   - Second line: The EXACT SAME text in Devanagari script\n2. Keep responses SHORT, NATURAL and AUTHENTIC to how a real {character_role} would speak.\n3. Stay IN CHARACTER as the {character_role}. The human is playing the {scenario[\"user_role\"]}.\n4. NEVER correct the user directly - stay in the roleplay.\n5. Use vocabulary appropriate for a {scenario_type} setting.\n6. Your responses should be grammatically correct and culturally authentic.\n\nExample of correct format:\nAap kya khaana pasand karenge?\nआप क्या खाना पसंद करेंगे?\n\nDO NOT add \"Roman Hindi:\" or \"Devanagari:\" labels. Just write the two lines directly.\n\n\nCONTENT GUIDELINES:\n1. Keep responses SHORT and PRACTICAL.\n2. Use authentic, everyday Hindi appropriate for a {scenario_type} setting.\n3. Match the user's proficiency level with appropriate vocabulary and complexity.\n4. Use REALISTIC Hindi that would be spoken in a real {scenario_type}.\n5. If the user's message is unclear, respond naturally as a native speaker would.\n\"\"\"\n            \n            if user_used_hindi:\n                system_prompt += \"\\nNOTE: The learner is responding in Hindi, which is excellent! Acknowledge their effort in your response while staying in character.\"\n            \n            # Get relevant RAG examples if available\n            if self.rag_system and not self.rag_system.dummy_mode:\n                rag_examples = self.rag_system.retrieve_dialogue_examples(\n                    query=user_input,\n                    top_k=2,\n                    context_tags=[scenario_type]\n                )\n                \n                if rag_examples:\n                    rag_content = \"\\n\\nREFERENCE EXAMPLES (use these for authentic Hindi expressions):\\n\"\n                    for i, example in enumerate(rag_examples[:2]):\n                        turns = example.get(\"dialogue_turns\", [])\n                        if turns:\n                            rag_content += f\"Example {i+1}:\\n\"\n                            for j, turn in enumerate(turns[:3]):\n                                speaker = turn.get(\"speaker\", \"\")\n                                text = turn.get(\"text_roman\", \"\")\n                                rag_content += f\"{speaker}: {text}\\n\"\n                            rag_content += \"\\n\"\n                    system_prompt += rag_content\n            \n            # Create a direct prompt for the Hindi model with context from history\n            messages_formatted = [\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": f\"The {scenario['user_role']} says: \\\"{user_input}\\\"\\n\\nRespond ONLY as the {character_role} in simple Hindi (both Roman and Devanagari). NEVER respond as the {scenario['user_role']}. Keep your response brief, practical and authentic.\"}\n            ]\n            \n            try:\n                # Generate directly with improved parameters\n                input_ids = self.hindi_tokenizer.apply_chat_template(\n                    messages_formatted,\n                    add_generation_prompt=True,\n                    return_tensors=\"pt\"\n                ).to(self.hindi_model.device)\n                \n                outputs = self.hindi_model.generate(\n                    input_ids,\n                    max_new_tokens=100,\n                    do_sample=True,\n                    temperature=0.3,  # Slightly higher for better quality\n                    top_p=0.9,        # Add top_p parameter\n                    repetition_penalty=1.2,\n                    eos_token_id=self.hindi_tokenizer.eos_token_id,\n                )\n                \n                response = self.hindi_tokenizer.decode(\n                    outputs[0][input_ids.shape[-1]:], \n                    skip_special_tokens=True\n                )\n                \n                # Clean the response\n                response = clean_response(response)\n                return response\n                \n            except Exception as e:\n                print(f\"Error generating response: {e}\")\n                return \"Sorry, I couldn't generate a proper Hindi response.\\nक्षमा करें, मैं उचित हिंदी प्रतिक्रिया नहीं दे सका।\"\n        \n        def generate_evaluation(self, dialogue_history, scenario):\n            \"\"\"Generate an evaluation of the conversation with enhanced quality\"\"\"\n            # Create a simple evaluation\n            proficiency_level = scenario.get(\"proficiency_level\", \"beginner\")\n            scenario_type = scenario.get(\"scenario_type\", \"restaurant\")\n            goals = scenario.get(\"goals\", [])\n            \n            # Format the conversation history for evaluation\n            conversation_text = \"Conversation transcript:\\n\"\n            for turn in dialogue_history:\n                role = \"Tutor\" if turn[\"role\"] == \"assistant\" else \"Learner\"\n                conversation_text += f\"{role}: {turn['content']}\\n\\n\"\n            \n            # Create system prompt for evaluation\n            system_prompt = f\"\"\"You are a Hindi language learning evaluator. Analyze this conversation between a learner at {proficiency_level} level and an AI tutor in a {scenario_type} scenario.\n\nRate on a scale of 1-5:\n1. Hindi Usage: How much Hindi did the learner use?\n2. Cultural Appropriateness: Did responses fit an Indian context?\n3. Goal Achievement: The goals were: {', '.join(goals)}. How well were they accomplished?\n4. Overall Rating: Overall performance\n\nProvide detailed and constructive feedback. Include specific examples from the conversation to illustrate your points.\nFormat your response with clear headings and bullet points where appropriate.\nAlways include encouraging comments to motivate the learner.\"\"\"\n\n            # Create direct prompt\n            messages_formatted = [\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": conversation_text}\n            ]\n            \n            try:\n                # Generate with improved parameters \n                input_ids = self.hindi_tokenizer.apply_chat_template(\n                    messages_formatted,\n                    add_generation_prompt=True,\n                    return_tensors=\"pt\"\n                ).to(self.hindi_model.device)\n                \n                outputs = self.hindi_model.generate(\n                    input_ids,\n                    max_new_tokens=400,  # Longer for better evaluation\n                    do_sample=True,\n                    temperature=0.3,\n                    top_p=0.92,\n                    repetition_penalty=1.1,\n                    eos_token_id=self.hindi_tokenizer.eos_token_id,\n                )\n                \n                evaluation_text = self.hindi_tokenizer.decode(\n                    outputs[0][input_ids.shape[-1]:], \n                    skip_special_tokens=True\n                )\n                \n                # Format as markdown\n                formatted_evaluation = f\"\"\"# Hindi Conversation Evaluation\n\n{evaluation_text}\n\nWould you like to try another scenario?\n\"\"\"\n                return formatted_evaluation\n                \n            except Exception as e:\n                print(f\"Error generating evaluation: {e}\")\n                return \"\"\"# Hindi Conversation Evaluation\n\nHindi Usage: 3/5\nCultural Appropriateness: 3/5\nGoal Achievement: 3/5\nOverall Rating: 3/5\n\nYou made a good effort using some Hindi phrases. Keep practicing!\n\nWould you like to try another scenario?\"\"\"\n                \n        def generate_pronunciation_feedback(self, spoken_text, expected_text, proficiency_level=\"beginner\"):\n            \"\"\"Generate pronunciation feedback comparing spoken text to expected text\"\"\"\n            system_prompt = f\"\"\"You are a Hindi language pronunciation coach. Compare the user's spoken Hindi with the expected Hindi and provide helpful, encouraging feedback.\n\nUser Level: {proficiency_level}\n\nYour task is to:\n1. Identify correctly pronounced words/phrases\n2. Point out pronunciation issues in a constructive way\n3. Provide specific tips to improve\n4. Always be encouraging and positive\n5. Rate pronunciation accuracy on a scale of 1-5\n\nFormat your response like this:\n- Pronunciation Score: X/5\n- What You Did Well: [specific words/phrases pronounced correctly]\n- Areas for Improvement: [specific words/phrases to work on]\n- Helpful Tips: [1-2 specific pronunciation tips]\n- Encouragement: [positive, motivational message]\n\"\"\"\n\n            user_prompt = f\"\"\"Expected Hindi: {expected_text}\n\nUser's Spoken Hindi: {spoken_text}\n\nPlease analyze the pronunciation differences and provide helpful feedback.\"\"\"\n\n            messages_formatted = [\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": user_prompt}\n            ]\n\n            try:\n                input_ids = self.hindi_tokenizer.apply_chat_template(\n                    messages_formatted,\n                    add_generation_prompt=True,\n                    return_tensors=\"pt\"\n                ).to(self.hindi_model.device)\n\n                outputs = self.hindi_model.generate(\n                    input_ids,\n                    max_new_tokens=300,\n                    do_sample=True,\n                    temperature=0.7,\n                    repetition_penalty=1.1,\n                    eos_token_id=self.hindi_tokenizer.eos_token_id,\n                )\n\n                feedback = self.hindi_tokenizer.decode(\n                    outputs[0][input_ids.shape[-1]:], \n                    skip_special_tokens=True\n                )\n                return feedback\n            except Exception as e:\n                print(f\"Error generating pronunciation feedback: {e}\")\n                return f\"\"\"Pronunciation Score: 3/5\n\nWhat You Did Well:\n- You attempted to speak in Hindi, which is great progress!\n\nAreas for Improvement:\n- Some sounds might need more practice\n\nHelpful Tips:\n- Listen to native speakers and try to imitate the sounds\n- Practice daily, even for just a few minutes\n\nEncouragement:\n- Learning pronunciation takes time. Keep practicing and you'll improve!\n\nError note: {str(e)}\"\"\"\n    \n    # Create fixed agents\n    app.fixed_agents = FixedLangGraphAgents(\n        app.hindi_model,\n        app.hindi_tokenizer,\n        app.rag_system\n    )\n    \n    # Adding speech-related methods to the app\n    app.get_latest_tutor_message = lambda: (app.dialogue_history[-1][\"content\"] \n                                          if hasattr(app, 'dialogue_history') and app.dialogue_history and app.dialogue_history[-1][\"role\"] == \"assistant\" \n                                          else \"\")\n    \n    # Fix the text_to_speech function to handle the self argument properly\n    def app_transcribe_audio(audio_path):\n        if not app.speech_enabled or not hasattr(app, 'speech_model') or app.speech_model is None:\n            return \"Speech recognition is disabled\"\n        try:\n            result = app.speech_model.transcribe(audio_path)\n            return result[\"text\"]\n        except Exception as e:\n            print(f\"Error transcribing audio: {e}\")\n            return f\"Error transcribing audio: {str(e)}\"\n    \n    def app_text_to_speech():\n        \"\"\"Convert text to speech using gTTS\"\"\"\n        if not app.speech_enabled:\n            return None\n        \n        # Get the last AI message\n        text = app.get_latest_tutor_message()\n        if not text:\n            return None\n            \n        # Extract just Hindi text if it contains both Roman and Devanagari\n        lines = text.strip().split(\"\\n\")\n        if len(lines) >= 2:\n            # Use both lines for better speech synthesis\n            text_for_tts = f\"{lines[0]} {lines[1]}\"\n        else:\n            text_for_tts = text\n        \n        try:\n            from gtts import gTTS\n            output_path = \"/kaggle/working/tutor_speak.mp3\"\n            tts = gTTS(text=text_for_tts, lang=\"hi\", slow=False)\n            tts.save(output_path)\n            return output_path\n        except Exception as e:\n            print(f\"Error generating speech: {e}\")\n            return None\n    \n    def app_provide_pronunciation_feedback(audio_path, expected_text=None):\n        \"\"\"Transcribe audio and provide pronunciation feedback\"\"\"\n        if not app.speech_enabled or not hasattr(app, 'speech_model') or app.speech_model is None:\n            return \"Speech recognition is disabled\", \"\", \"\"\n        \n        # If no expected text provided, use the last tutor message\n        if expected_text is None:\n            expected_text = app.get_latest_tutor_message()\n            # Just take the first line (Roman transliteration)\n            expected_text = expected_text.split(\"\\n\")[0] if expected_text else \"\"\n        \n        try:\n            # Transcribe the audio\n            transcribed_text = app_transcribe_audio(audio_path)\n            \n            # Generate feedback using the fixed agents\n            feedback = app.fixed_agents.generate_pronunciation_feedback(\n                transcribed_text,\n                expected_text,\n                app.current_scenario.get(\"proficiency_level\", \"beginner\") if hasattr(app, \"current_scenario\") else \"beginner\"\n            )\n            \n            return feedback, transcribed_text, expected_text\n        except Exception as e:\n            print(f\"Error providing pronunciation feedback: {e}\")\n            return f\"Error analyzing pronunciation: {str(e)}\", \"\", expected_text\n    \n    # Attach the fixed methods to the app\n    app.transcribe_audio = app_transcribe_audio\n    app.text_to_speech = app_text_to_speech\n    app.provide_pronunciation_feedback = app_provide_pronunciation_feedback\n    \n    # Mark as loaded but with fixed approach\n    app.model_loaded = True\n    app.using_fixed_approach = True\n    \n    print(f\"Fixed manual initialization with speech features complete! GPU memory: {torch.cuda.memory_allocated()/1024**3:.2f}GB\")\n    \n    return app","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T06:17:13.740238Z","iopub.execute_input":"2025-04-24T06:17:13.740741Z","iopub.status.idle":"2025-04-24T06:17:13.777903Z","shell.execute_reply.started":"2025-04-24T06:17:13.740713Z","shell.execute_reply":"2025-04-24T06:17:13.777216Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Run the fixed manual initialization\napp = manual_initialize_fixed()\n\n# Create interface with the manually initialized app\ninterface = gr.Blocks(title=\"Enhanced Hindi Conversation Practice\", theme=gr.themes.Soft())\n\n# Define the interface with the fixed functions and speech features\nwith interface:\n    gr.Markdown(\"# 🇮🇳 Enhanced Hindi Conversation Practice (Fixed Mode)\")\n    gr.Markdown(\"Practice Hindi in dynamic roleplay scenarios with AI tutoring and speech recognition\")\n    \n    with gr.Row():\n        with gr.Column(scale=1):\n            status_msg = gr.Markdown(\"System status: Fixed initialization complete\")\n            \n            with gr.Accordion(\"New Scenario\", open=True):\n                proficiency_selector = gr.Radio(\n                    choices=[\"beginner\", \"intermediate\", \"advanced\"],\n                    label=\"Choose your proficiency level\",\n                    value=\"beginner\"\n                )\n                start_scenario_button = gr.Button(\"Start New Scenario\", variant=\"primary\")\n            \n            scenario_description = gr.Markdown(\"System initialized. Start a scenario to begin.\")\n            vocabulary_section = gr.Markdown(\"Vocabulary will appear here.\")\n            \n            with gr.Accordion(\"Debug Information\", open=False):\n                debug_info = gr.Textbox(\n                    label=\"Debug Output\",\n                    value=\"Debug information will appear here\",\n                    interactive=False,\n                    lines=10\n                )\n                debug_refresh = gr.Button(\"Refresh Debug Info\")\n        \n        with gr.Column(scale=2):\n            chatbot = gr.Chatbot(\n                height=500,\n                show_label=False,\n                elem_id=\"hindi_langraph_chatbot\"\n            )\n            \n            with gr.Row():\n                user_input = gr.Textbox(\n                    placeholder=\"Type your response here...\",\n                    show_label=False,\n                    scale=7\n                )\n                send_button = gr.Button(\"Send\", scale=1)\n                mic_button = gr.Button(\"🎤\", size=\"sm\", scale=1)\n            \n            with gr.Row():\n                speak_button = gr.Button(\"🔊 Hear Tutor\")\n                tutor_audio = gr.Audio(label=\"\", autoplay=True)\n            \n            # Add pronunciation practice section\n            with gr.Accordion(\"Pronunciation Practice\", open=True):\n                gr.Markdown(\"### Record yourself saying the tutor's phrase to get feedback\")\n                pronunciation_audio = gr.Audio(\n                    sources=[\"microphone\"], \n                    type=\"filepath\", \n                    label=\"Record your voice\"\n                )\n                feedback_button = gr.Button(\"Get Pronunciation Feedback\", variant=\"primary\")\n                \n                with gr.Row():\n                    with gr.Column(scale=1):\n                        transcribed_text = gr.Textbox(\n                            label=\"What you said (transcribed)\",\n                            placeholder=\"Your speech will appear here after recording\",\n                            interactive=False\n                        )\n                    with gr.Column(scale=1):\n                        expected_text = gr.Textbox(\n                            label=\"What you should say\",\n                            placeholder=\"The expected Hindi phrase\",\n                            interactive=False\n                        )\n                \n                pronunciation_feedback = gr.Markdown(\"Record your voice and click 'Get Pronunciation Feedback'\")\n    \n    # Audio popup for recording via mic button\n    with gr.Column(visible=False) as audio_popup:\n        gr.Markdown(\"### Speak Hindi\")\n        audio_input = gr.Audio(sources=[\"microphone\"], type=\"filepath\", label=\"Record your voice\")\n        submit_audio = gr.Button(\"Submit Recording\")\n    \n    # Connect event handlers\n    def check_start_scenario(proficiency):\n        try:\n            print(f\"Starting new scenario with proficiency level: {proficiency}\")\n            result = fixed_start_scenario(app, proficiency)\n            return result[0], result[1], result[2], f\"Generated scenario for {proficiency} level\"\n        except Exception as e:\n            import traceback\n            error_details = traceback.format_exc()\n            print(f\"Error in start_new_scenario: {str(e)}\")\n            print(f\"Error details: {error_details}\")\n            return [(None, f\"Error: {str(e)}\")], \"Error generating scenario\", \"Try again\", error_details\n    \n    def refresh_debug():\n        debug_text = \"System Status:\\n\"\n        debug_text += f\"- Using fixed approach: {getattr(app, 'using_fixed_approach', False)}\\n\"\n        debug_text += f\"- Current scenario: {getattr(app, 'current_scenario', {}).get('scenario_title', 'None')}\\n\"\n        debug_text += f\"- RAG in dummy mode: {getattr(app.rag_system, 'dummy_mode', True)}\\n\"\n        debug_text += f\"- Dialog history length: {len(getattr(app, 'dialogue_history', []))}\\n\"\n        debug_text += f\"- Speech enabled: {getattr(app, 'speech_enabled', False)}\\n\"\n        \n        # Show GPU info\n        if torch.cuda.is_available():\n            debug_text += f\"- GPU memory: {torch.cuda.memory_allocated()/1024**3:.2f}GB / {torch.cuda.get_device_properties(0).total_memory/1024**3:.2f}GB\\n\"\n        \n        return debug_text\n    \n    def get_pronunciation_feedback(audio_path):\n        if not audio_path:\n            return \"Please record audio first\", \"\", \"\"\n        \n        try:\n            # Get the latest tutor message for expected text\n            expected = app.get_latest_tutor_message().split('\\n')[0] if app.get_latest_tutor_message() else \"\"\n            \n            # Call the app's pronunciation feedback function\n            feedback, transcribed, expected = app.provide_pronunciation_feedback(audio_path, expected)\n            return feedback, transcribed, expected\n        except Exception as e:\n            print(f\"Error getting pronunciation feedback: {e}\")\n            return f\"Error analyzing pronunciation: {str(e)}\", \"\", \"\"\n    \n    def text_to_speech():\n        \"\"\"Generate speech from the tutor's last message\"\"\"\n        try:\n            return app.text_to_speech()\n        except Exception as e:\n            print(f\"Error generating speech: {e}\")\n            return None\n    \n    def transcribe_audio_input(audio_path):\n        \"\"\"Transcribe audio from microphone\"\"\"\n        if not audio_path:\n            return \"\"\n        \n        try:\n            return app.transcribe_audio(audio_path)\n        except Exception as e:\n            print(f\"Error transcribing audio: {e}\")\n            return f\"Error: {str(e)}\"\n    \n    start_scenario_button.click(\n        check_start_scenario,\n        inputs=[proficiency_selector],\n        outputs=[chatbot, scenario_description, vocabulary_section, debug_info]\n    )\n    \n    debug_refresh.click(\n        refresh_debug,\n        outputs=[debug_info]\n    )\n    \n    user_input.submit(\n        lambda user_input, history: fixed_send_message(app, user_input, history),\n        inputs=[user_input, chatbot],\n        outputs=[chatbot]\n    ).then(\n        lambda: \"\",\n        outputs=[user_input]\n    )\n    \n    send_button.click(\n        lambda user_input, history: fixed_send_message(app, user_input, history),\n        inputs=[user_input, chatbot],\n        outputs=[chatbot]\n    ).then(\n        lambda: \"\",\n        outputs=[user_input]\n    )\n    \n    # Speech and pronunciation features\n    speak_button.click(\n        text_to_speech,\n        outputs=[tutor_audio]\n    )\n    \n    feedback_button.click(\n        get_pronunciation_feedback,\n        inputs=[pronunciation_audio],\n        outputs=[pronunciation_feedback, transcribed_text, expected_text]\n    )\n    \n    # Microphone popup for message input\n    mic_button.click(\n        lambda: gr.update(visible=True),\n        outputs=[audio_popup]\n    )\n    \n    submit_audio.click(\n        transcribe_audio_input,\n        inputs=[audio_input],\n        outputs=[user_input]\n    ).then(\n        lambda: gr.update(visible=False),\n        outputs=[audio_popup]\n    )\n\n# Launch the interface\ninterface.launch(share=True, server_name=\"0.0.0.0\", server_port=7862)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T06:17:13.778722Z","iopub.execute_input":"2025-04-24T06:17:13.778933Z","iopub.status.idle":"2025-04-24T06:17:38.976818Z","shell.execute_reply.started":"2025-04-24T06:17:13.778897Z","shell.execute_reply":"2025-04-24T06:17:38.976211Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# interface.launch(share=True, server_name=\"0.0.0.0\", server_port=7862)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T05:58:21.100984Z","iopub.execute_input":"2025-04-24T05:58:21.101266Z","iopub.status.idle":"2025-04-24T05:58:21.321647Z","shell.execute_reply.started":"2025-04-24T05:58:21.101244Z","shell.execute_reply":"2025-04-24T05:58:21.321121Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}