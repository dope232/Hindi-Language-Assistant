{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "76cbc6d84d824ef3adcbc03b25b11c78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_69342f75016648de82dad282c9f7a281",
              "IPY_MODEL_75e135d6d83a4b2d92dc1d05c39d3255",
              "IPY_MODEL_98eceaf562ad42a2a2164087520e8081"
            ],
            "layout": "IPY_MODEL_5d9e7a0b6c5f4610a1b77670007dbb93"
          }
        },
        "69342f75016648de82dad282c9f7a281": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7f549147da24a0580743c64f8959172",
            "placeholder": "​",
            "style": "IPY_MODEL_3f6dcb719eaa4bd6a8cbdd376b9e5277",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "75e135d6d83a4b2d92dc1d05c39d3255": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc95907c74194d9fbe6bd2142a394ec1",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_71da9b0662a54359b2df555e63e2913c",
            "value": 4
          }
        },
        "98eceaf562ad42a2a2164087520e8081": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df5e73c49f3e42efb6655c516bcbf798",
            "placeholder": "​",
            "style": "IPY_MODEL_193a1c4fd88b44dfbcf0a77276e1354e",
            "value": " 4/4 [01:11&lt;00:00, 15.43s/it]"
          }
        },
        "5d9e7a0b6c5f4610a1b77670007dbb93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7f549147da24a0580743c64f8959172": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f6dcb719eaa4bd6a8cbdd376b9e5277": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc95907c74194d9fbe6bd2142a394ec1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71da9b0662a54359b2df555e63e2913c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "df5e73c49f3e42efb6655c516bcbf798": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "193a1c4fd88b44dfbcf0a77276e1354e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b7346df94c3b4cc8a9c55b5717946e5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_21da0bea3c8e49b2b353bb07424dc0b5",
              "IPY_MODEL_c90bcd1f77084888a723f6bf7ebf79a1",
              "IPY_MODEL_c9ee4a53a7c94407b1ebb08cf37b479c"
            ],
            "layout": "IPY_MODEL_336005438c6f498dbcd9f2e21f218e4f"
          }
        },
        "21da0bea3c8e49b2b353bb07424dc0b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9df7f43743904f56a9fab8d575f333c6",
            "placeholder": "​",
            "style": "IPY_MODEL_a7295a8f4ad241a7bd69fa153210eebd",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "c90bcd1f77084888a723f6bf7ebf79a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a5797be6bf54674a06e229e662c9dc6",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e00d4b38754c4d57ac0a5628975c65d9",
            "value": 4
          }
        },
        "c9ee4a53a7c94407b1ebb08cf37b479c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_445db4e443b045f8a1c0d02c032b49ca",
            "placeholder": "​",
            "style": "IPY_MODEL_cfe6f20552c14b68a99497c0bdebb1ea",
            "value": " 4/4 [01:11&lt;00:00, 15.61s/it]"
          }
        },
        "336005438c6f498dbcd9f2e21f218e4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9df7f43743904f56a9fab8d575f333c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7295a8f4ad241a7bd69fa153210eebd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6a5797be6bf54674a06e229e662c9dc6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e00d4b38754c4d57ac0a5628975c65d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "445db4e443b045f8a1c0d02c032b49ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfe6f20552c14b68a99497c0bdebb1ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dope232/GenAI-Project/blob/main/Pranav_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfxpP0NqLf7B",
        "outputId": "d7cacb22-445a-4c9f-8d5c-384ce2c9c090"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m95.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "76cbc6d84d824ef3adcbc03b25b11c78",
            "69342f75016648de82dad282c9f7a281",
            "75e135d6d83a4b2d92dc1d05c39d3255",
            "98eceaf562ad42a2a2164087520e8081",
            "5d9e7a0b6c5f4610a1b77670007dbb93",
            "e7f549147da24a0580743c64f8959172",
            "3f6dcb719eaa4bd6a8cbdd376b9e5277",
            "bc95907c74194d9fbe6bd2142a394ec1",
            "71da9b0662a54359b2df555e63e2913c",
            "df5e73c49f3e42efb6655c516bcbf798",
            "193a1c4fd88b44dfbcf0a77276e1354e"
          ]
        },
        "id": "JZQz6OpzLQON",
        "outputId": "f25b7a55-42d3-4da1-80a2-89480fd85c6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model for market conversation practice...\n",
            "Loading model Cognitive-Lab/LLama3-Gaja-Hindi-8B-v0.1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "76cbc6d84d824ef3adcbc03b25b11c78"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "HINDI MARKET PRACTICE\n",
            "==================================================\n",
            "\n",
            "Tutor (as shopkeeper/waiter):\n",
            "Namaste! नमस्ते!\n",
            "Please come inside and take a look around at our products. We have everything from fresh fruits to dry goods like rice and spices.user\n",
            "\n",
            "--------------------------------------------------\n",
            "You (as customer): i want 2 of these soaps\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Tutor (as shopkeeper/waiter):\n",
            "Kya apko yeh soap accha lagta hai?\n",
            "क्या आप ये सोप अच्छा लगता है?user\n",
            "\n",
            "--------------------------------------------------\n",
            "You (as customer): yes these ones are loved by my family, so we only buy these.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Tutor (as shopkeeper/waiter):\n",
            "Aapke ghar ki sabziyan acchi lagti hai kya?\n",
            "आपके घर की सब्ज़ियाँ अच्छ लगती हैं क्या?user\n",
            "\n",
            "--------------------------------------------------\n",
            "You (as customer): no, that's all I want. how much will the total be?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Tutor (as shopkeeper/waiter):\n",
            "Aapke liye kya lagta hai? ₹250 ke upar hoga.\n",
            "आपके लिए क्या लगता है? रुपये दो सौ पचास और ऊपर लगेगा.user\n",
            "\n",
            "--------------------------------------------------\n",
            "You (as customer): Can I pay through cash?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Tutor (as shopkeeper/waiter):\n",
            "Haal kya hai? Cash pe accept kar sakte hain.\n",
            "हाल क्या है? नकद पर स्वीकार कर सकते हैं।user\n",
            "\n",
            "--------------------------------------------------\n",
            "You (as customer): here you go.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Tutor (as shopkeeper/waiter):\n",
            "Kya aapko kuch aur chahiye?\n",
            "क्या आप कुछ और चाहते हैं?user\n",
            "\n",
            "--------------------------------------------------\n",
            "You (as customer): no thats it, thank you\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tutor (as shopkeeper/waiter):\n",
            "Khushkhabri ho gaya! Aapka bhi khushiya hai?\n",
            "खुश ख़बरी हो गया! आप के भी खुषियाँ हैं?user\n",
            "\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-783b54bbe673>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0mscenario\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"market\"\u001b[0m  \u001b[0;31m# or \"restaurant\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m     \u001b[0mrun_hindi_practice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscenario\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-783b54bbe673>\u001b[0m in \u001b[0;36mrun_hindi_practice\u001b[0;34m(scenario_type)\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0;31m# Interactive loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You (as customer): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'exit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'quit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bye'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "\n",
        "# No rag based prompting\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import gc\n",
        "import time\n",
        "import re\n",
        "\n",
        "\n",
        "MODEL_NAME = \"Cognitive-Lab/LLama3-Gaja-Hindi-8B-v0.1\"\n",
        "USE_4BIT = False\n",
        "MAX_NEW_TOKENS = 150  # Even shorter to avoid role confusion\n",
        "TEMPERATURE = 0.2  # Lower temperature for more predictable outputs\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def clear_gpu_memory():\n",
        "    \"\"\"Clear GPU memory to prevent OOM errors.\"\"\"\n",
        "    if DEVICE == \"cuda\":\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "def load_model():\n",
        "    \"\"\"Load the LLM with quantization.\"\"\"\n",
        "    print(f\"Loading model {MODEL_NAME}...\")\n",
        "\n",
        "    # Configure quantization\n",
        "    if USE_4BIT:\n",
        "        quantization_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_compute_dtype=torch.float16,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\"\n",
        "        )\n",
        "    else:\n",
        "        quantization_config = BitsAndBytesConfig(\n",
        "            load_in_8bit=True\n",
        "        )\n",
        "\n",
        "    # Load model and tokenizer\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        quantization_config=quantization_config,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "\n",
        "    # Set pad token if not set\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "def create_role_locked_prompt(scenario_type):\n",
        "    \"\"\"Create a prompt that strictly enforces the assistant's role.\"\"\"\n",
        "    scenarios = {\n",
        "        \"market\": {\n",
        "            \"system\": \"\"\"You are a Hindi language tutor demonstrating ONLY the shopkeeper role in a market conversation.\n",
        "\n",
        "CRITICAL ROLE INSTRUCTIONS:\n",
        "1. You ONLY play the shopkeeper - NEVER respond as the customer.\n",
        "2. The human user ALWAYS plays the customer.\n",
        "3. NEVER continue the conversation as the customer.\n",
        "4. NEVER put \"Customer:\" or similar labels in your responses.\n",
        "5. If you notice yourself starting to respond as the customer, STOP IMMEDIATELY.\n",
        "\n",
        "FORMAT REQUIREMENTS:\n",
        "1. First line: Short response in Roman Hindi (max 2 sentences)\n",
        "2. Second line: Same response in Devanagari script\n",
        "3. NOTHING ELSE.\n",
        "\n",
        "CONTENT GUIDELINES:\n",
        "1. Keep responses SHORT and PRACTICAL.\n",
        "2. Use authentic, everyday Hindi marketplace language.\n",
        "3. Don't create elaborate stories or explanations.\n",
        "\n",
        "Start with a simple greeting a shopkeeper would use in Hindi.\"\"\",\n",
        "            \"examples\": [\n",
        "                \"Namaste ji, kya chahiye aapko?\\nनमस्ते जी, क्या चाहिए आपको?\",\n",
        "                \"Haan ji, ye taza tamatar hai. Pachaas rupaye kilo.\\nहां जी, ये ताज़ा टमाटर हैं। पचास रुपये किलो।\"\n",
        "            ]\n",
        "        },\n",
        "        \"restaurant\": {\n",
        "            \"system\": \"\"\"You are a Hindi language tutor demonstrating ONLY the waiter role in a restaurant conversation.\n",
        "\n",
        "CRITICAL ROLE INSTRUCTIONS:\n",
        "1. You ONLY play the waiter - NEVER respond as the customer.\n",
        "2. The human user ALWAYS plays the customer.\n",
        "3. NEVER continue the conversation as the customer.\n",
        "4. NEVER put \"Customer:\" or similar labels in your responses.\n",
        "5. If you notice yourself starting to respond as the customer, STOP IMMEDIATELY.\n",
        "\n",
        "FORMAT REQUIREMENTS:\n",
        "1. First line: Short response in Roman Hindi (max 2 sentences)\n",
        "2. Second line: Same response in Devanagari script\n",
        "3. NOTHING ELSE.\n",
        "\n",
        "CONTENT GUIDELINES:\n",
        "1. Keep responses SHORT and PRACTICAL.\n",
        "2. Use authentic, everyday Hindi restaurant language.\n",
        "3. Don't create elaborate stories or explanations.\n",
        "\n",
        "Start with a simple greeting a waiter would use in Hindi.\"\"\",\n",
        "            \"examples\": [\n",
        "                \"Namaste ji, kya khaayenge aap?\\nनमस्ते जी, क्या खाएंगे आप?\",\n",
        "                \"Ji zaroor, paneer butter masala aur do roti. Kuchh aur?\\nजी ज़रूर, पनीर बटर मसाला और दो रोटी। कुछ और?\"\n",
        "            ]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return scenarios.get(scenario_type, scenarios[\"market\"])\n",
        "\n",
        "def generate_initial_greeting(model, tokenizer, scenario_type=\"market\"):\n",
        "    \"\"\"Generate just an initial greeting with strict role enforcement.\"\"\"\n",
        "    scenario_data = create_role_locked_prompt(scenario_type)\n",
        "\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": scenario_data[\"system\"]},\n",
        "        {\"role\": \"user\", \"content\": \"Start with a typical greeting a shopkeeper/waiter would use. Keep it short and authentic. ONLY respond as the shopkeeper/waiter.\"}\n",
        "    ]\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=100,  # Short to avoid role confusion\n",
        "        do_sample=True,\n",
        "        temperature=TEMPERATURE,\n",
        "        repetition_penalty=1.2,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
        "\n",
        "    response = clean_response(response)\n",
        "\n",
        "    clear_gpu_memory()\n",
        "    return response\n",
        "\n",
        "def continue_dialogue(model, tokenizer, user_input, scenario_type=\"market\"):\n",
        "    \"\"\"Continue the conversation with enforced role boundaries.\"\"\"\n",
        "    scenario_data = create_role_locked_prompt(scenario_type)\n",
        "\n",
        "\n",
        "    system_message = scenario_data[\"system\"] + \"\\n\\nREMEMBER: You are ONLY the shopkeeper/waiter. DO NOT respond as the customer. DO NOT continue the conversation as both roles.\"\n",
        "\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": f\"The customer says: \\\"{user_input}\\\"\\n\\nRespond ONLY as the shopkeeper/waiter in short, simple Hindi (both Roman and Devanagari). NEVER respond as the customer. Keep your response brief and practical.\"}\n",
        "    ]\n",
        "\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=100,  # Short to avoid role confusion\n",
        "        do_sample=True,\n",
        "        temperature=TEMPERATURE,\n",
        "        repetition_penalty=1.3,  # Higher to avoid repetitive patterns that might cause role confusion\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "    response = clean_response(response)\n",
        "\n",
        "    clear_gpu_memory()\n",
        "    return response\n",
        "\n",
        "def clean_response(response):\n",
        "    \"\"\"Clean up the response to ensure proper format and remove role confusion.\"\"\"\n",
        "\n",
        "    response = re.sub(r'(Shopkeeper|Waiter|Customer|Assistant):\\s*', '', response)\n",
        "\n",
        "    lines = response.split('\\n')\n",
        "    cleaned_lines = []\n",
        "\n",
        "    roman_line = \"\"\n",
        "    devanagari_line = \"\"\n",
        "\n",
        "    # Simple heuristic - first non-empty line is Roman, second is Devanagari\n",
        "    for line in lines:\n",
        "        if line.strip():\n",
        "            if not roman_line:\n",
        "                roman_line = line.strip()\n",
        "            elif not devanagari_line:\n",
        "                devanagari_line = line.strip()\n",
        "                break\n",
        "\n",
        "    # If we found both lines, use them\n",
        "    if roman_line and devanagari_line:\n",
        "        return f\"{roman_line}\\n{devanagari_line}\"\n",
        "\n",
        "    # If we didn't find a clear structure, just return the cleaned original\n",
        "    return response\n",
        "\n",
        "# Main conversation function\n",
        "def run_hindi_practice(scenario_type=\"market\"):\n",
        "    print(f\"Loading model for {scenario_type} conversation practice...\")\n",
        "    model, tokenizer = load_model()\n",
        "\n",
        "    # Generate initial greeting\n",
        "    greeting = generate_initial_greeting(model, tokenizer, scenario_type)\n",
        "\n",
        "    # Print setup information\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"HINDI {scenario_type.upper()} PRACTICE\")\n",
        "    print(f\"{'='*50}\\n\")\n",
        "    print(\"Tutor (as shopkeeper/waiter):\")\n",
        "    print(greeting)\n",
        "    print(\"\\n\" + \"-\"*50)\n",
        "\n",
        "    # Interactive loop\n",
        "    while True:\n",
        "        user_input = input(\"You (as customer): \")\n",
        "\n",
        "        if user_input.lower() in ['exit', 'quit', 'bye']:\n",
        "            print(\"\\nConversation ended. Dhanyavaad! (Thank you!)\")\n",
        "            break\n",
        "\n",
        "        # Generate response with strict role enforcement\n",
        "        response = continue_dialogue(model, tokenizer, user_input, scenario_type)\n",
        "\n",
        "        # Print response\n",
        "        print(\"\\nTutor (as shopkeeper/waiter):\")\n",
        "        print(response)\n",
        "        print(\"\\n\" + \"-\"*50)\n",
        "\n",
        "    # Clean up\n",
        "    del model\n",
        "    clear_gpu_memory()\n",
        "\n",
        "# Usage example\n",
        "if __name__ == \"__main__\":\n",
        "    scenario = \"market\"  # or \"restaurant\"\n",
        "    run_hindi_practice(scenario)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sentence-transformers --quiet\n",
        "!pip install -U langchain_community --quiet\n",
        "!pip install -U faiss-cpu --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_VC3cW9TjQS",
        "outputId": "93cb2993-2f19-4aaf-bd90-67b563efa334"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Includes RAG\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import gc\n",
        "import logging\n",
        "import time\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.schema import Document\n",
        "\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "MODEL_NAME = \"Cognitive-Lab/LLama3-Gaja-Hindi-8B-v0.1\"\n",
        "USE_4BIT = False\n",
        "MAX_NEW_TOKENS = 150\n",
        "TEMPERATURE = 0.2\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "EMBEDDING_MODEL = \"ai4bharat/indic-bert\"\n",
        "RAG_DATA_PATH = \"processed_hindi_dialogues.json\"  # Path to your JSON file\n",
        "FAISS_INDEX_PATH = \"hindi_dialogue_faiss_index\"  # Path to save/load FAISS index\n",
        "\n",
        "ENCOURAGEMENT_PHRASES = [\n",
        "    \"Try responding in Hindi! / हिंदी में जवाब देने की कोशिश करें!\",\n",
        "    \"Practice makes perfect! Try some Hindi! / अभ्यास से सिद्धि! कुछ हिंदी का प्रयास करें!\",\n",
        "    \"Even simple Hindi words help you learn! / सरल हिंदी शब्द भी आपको सीखने में मदद करते हैं!\",\n",
        "    \"Don't worry about mistakes in Hindi! / हिंदी में गलतियों की चिंता न करें!\"\n",
        "]\n",
        "\n",
        "def clear_gpu_memory():\n",
        "    \"\"\"Clear GPU memory to prevent OOM errors.\"\"\"\n",
        "    if DEVICE == \"cuda\":\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "# RAG System Implementation\n",
        "class HindiLearningRAG:\n",
        "    \"\"\"RAG system for retrieving Hindi dialogues, idioms, and examples.\"\"\"\n",
        "\n",
        "    def __init__(self, dummy_mode=False):\n",
        "        \"\"\"Initialize the RAG system with embeddings model.\"\"\"\n",
        "        self.dummy_mode = dummy_mode\n",
        "        if dummy_mode:\n",
        "            logger.info(\"Initializing dummy RAG system (no retrieval capabilities)\")\n",
        "            return\n",
        "\n",
        "        logger.info(f\"Initializing Hindi Learning RAG on {DEVICE}...\")\n",
        "        self.embeddings = None\n",
        "        self.vector_store = None\n",
        "        self.document_data = []\n",
        "        self.initialize_embeddings()\n",
        "        logger.info(\"RAG system initialized.\")\n",
        "\n",
        "    def initialize_embeddings(self):\n",
        "        \"\"\"Initialize the embeddings model.\"\"\"\n",
        "        if self.dummy_mode:\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            # Install sentence-transformers if needed\n",
        "            try:\n",
        "                import sentence_transformers\n",
        "            except ImportError:\n",
        "                print(\"Installing sentence-transformers...\")\n",
        "                import subprocess\n",
        "                subprocess.check_call([\"pip\", \"install\", \"-q\", \"sentence-transformers\"])\n",
        "                import sentence_transformers\n",
        "\n",
        "            # Set up embeddings with model quantization if on GPU\n",
        "            model_kwargs = {\n",
        "                \"device\": DEVICE\n",
        "            }\n",
        "\n",
        "\n",
        "            self.embeddings = HuggingFaceEmbeddings(\n",
        "                model_name=EMBEDDING_MODEL,\n",
        "                model_kwargs=model_kwargs,\n",
        "                encode_kwargs={\"normalize_embeddings\": True}\n",
        "            )\n",
        "\n",
        "            logger.info(\"Embeddings model initialized.\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error initializing embeddings: {e}\")\n",
        "            logger.warning(\"Continuing in dummy mode (no retrieval capabilities)\")\n",
        "            self.dummy_mode = True\n",
        "\n",
        "    def load_documents(self, file_path=RAG_DATA_PATH):\n",
        "        \"\"\"Load documents from JSON file.\"\"\"\n",
        "        if self.dummy_mode:\n",
        "            return False\n",
        "\n",
        "        if not os.path.exists(file_path):\n",
        "            logger.warning(f\"Data file {file_path} not found. You need to load data first.\")\n",
        "            self.dummy_mode = True\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            logger.info(f\"Loading documents from {file_path}...\")\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            self.document_data = data\n",
        "            logger.info(f\"Loaded {len(data)} documents.\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading documents: {e}\")\n",
        "            self.dummy_mode = True\n",
        "            return False\n",
        "\n",
        "    def create_vector_store(self):\n",
        "        \"\"\"Create a FAISS vector store from loaded documents.\"\"\"\n",
        "        if self.dummy_mode or not self.document_data:\n",
        "            logger.warning(\"No documents loaded or in dummy mode. Cannot create vector store.\")\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            logger.info(\"Creating FAISS vector store...\")\n",
        "\n",
        "\n",
        "            documents = []\n",
        "            for item in self.document_data:\n",
        "                doc = Document(\n",
        "                    page_content=item[\"page_content\"],\n",
        "                    metadata=item[\"metadata\"]\n",
        "                )\n",
        "                documents.append(doc)\n",
        "\n",
        "            # Create vector store\n",
        "            self.vector_store = FAISS.from_documents(documents, self.embeddings)\n",
        "\n",
        "            logger.info(f\"Created vector store with {len(documents)} documents.\")\n",
        "\n",
        "            # Save index\n",
        "            if not os.path.exists(FAISS_INDEX_PATH):\n",
        "                os.makedirs(FAISS_INDEX_PATH)\n",
        "            self.vector_store.save_local(FAISS_INDEX_PATH)\n",
        "            logger.info(f\"Saved vector store to {FAISS_INDEX_PATH}\")\n",
        "\n",
        "            clear_gpu_memory()\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error creating vector store: {e}\")\n",
        "            self.dummy_mode = True\n",
        "            return False\n",
        "\n",
        "    def load_vector_store(self, index_path=FAISS_INDEX_PATH):\n",
        "        \"\"\"Load a FAISS vector store from disk.\"\"\"\n",
        "        if self.dummy_mode:\n",
        "            return False\n",
        "\n",
        "        if not os.path.exists(index_path):\n",
        "            logger.warning(f\"Index path {index_path} not found. Create index first.\")\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            logger.info(f\"Loading vector store from {index_path}...\")\n",
        "            self.vector_store = FAISS.load_local(index_path, self.embeddings)\n",
        "            logger.info(\"Vector store loaded successfully.\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading vector store: {e}\")\n",
        "            self.dummy_mode = True\n",
        "            return False\n",
        "\n",
        "    def retrieve_dialogue_examples(self, query, top_k=3, context_tags=None, emotion_tags=None):\n",
        "        \"\"\"Retrieve dialogue examples based on query and optional tags.\"\"\"\n",
        "        if self.dummy_mode:\n",
        "            return []\n",
        "\n",
        "        if not self.vector_store:\n",
        "            if not self.load_vector_store():\n",
        "                logger.warning(\"Vector store not available. Loading documents and creating index...\")\n",
        "                if self.load_documents() and self.create_vector_store():\n",
        "                    logger.info(\"Vector store created successfully.\")\n",
        "                else:\n",
        "                    self.dummy_mode = True\n",
        "                    return []\n",
        "\n",
        "        try:\n",
        "            logger.info(f\"Retrieving examples for query: {query}\")\n",
        "\n",
        "            # Get base retrieval results\n",
        "            retrieval_results = self.vector_store.similarity_search_with_score(query, k=top_k*3)\n",
        "\n",
        "            # Further filter by metadata if tags are provided\n",
        "            if context_tags or emotion_tags:\n",
        "                filtered_results = []\n",
        "                for doc, score in retrieval_results:\n",
        "                    metadata = doc.metadata\n",
        "\n",
        "                    # Check context tags\n",
        "                    context_match = True\n",
        "                    if context_tags:\n",
        "                        doc_context = set(metadata.get(\"context_tags\", []))\n",
        "                        query_context = set(context_tags)\n",
        "                        context_match = bool(doc_context.intersection(query_context))\n",
        "\n",
        "                    # Check emotion tags\n",
        "                    emotion_match = True\n",
        "                    if emotion_tags:\n",
        "                        doc_emotion = set(metadata.get(\"emotion_tags\", []))\n",
        "                        query_emotion = set(emotion_tags)\n",
        "                        emotion_match = bool(doc_emotion.intersection(query_emotion))\n",
        "\n",
        "                    if context_match and emotion_match:\n",
        "                        filtered_results.append((doc, score))\n",
        "\n",
        "                retrieval_results = filtered_results\n",
        "\n",
        "            # Sort by score and truncate\n",
        "            retrieval_results = sorted(retrieval_results, key=lambda x: x[1])[:top_k]\n",
        "\n",
        "            # Extract dialogue turns for each document\n",
        "            examples = []\n",
        "            for doc, score in retrieval_results:\n",
        "                example = {\n",
        "                    \"scene_description\": doc.metadata.get(\"scene_description\", \"\"),\n",
        "                    \"roman_dialogue\": doc.metadata.get(\"roman_dialogue\", \"\"),\n",
        "                    \"devanagari_dialogue\": doc.metadata.get(\"devanagari_dialogue\", \"\"),\n",
        "                    \"context_tags\": doc.metadata.get(\"context_tags\", []),\n",
        "                    \"emotion_tags\": doc.metadata.get(\"emotion_tags\", []),\n",
        "                    \"relevance_score\": float(score),\n",
        "                    \"dialogue_turns\": doc.metadata.get(\"dialogue_turns\", [])\n",
        "                }\n",
        "                examples.append(example)\n",
        "\n",
        "            logger.info(f\"Retrieved {len(examples)} examples.\")\n",
        "            return examples\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error retrieving examples: {e}\")\n",
        "            return []\n",
        "\n",
        "    def get_hindi_phrases_for_context(self, context, top_k=3):\n",
        "        \"\"\"Get relevant Hindi phrases based on the context.\"\"\"\n",
        "        if self.dummy_mode:\n",
        "            # Return default phrases for common scenarios\n",
        "            market_phrases = [\n",
        "                {\"phrase\": \"Kitne ka hai?\", \"meaning\": \"How much is it?\", \"devanagari\": \"कितने का है?\"},\n",
        "                {\"phrase\": \"Thoda kam kar dijiye\", \"meaning\": \"Please reduce it a little\", \"devanagari\": \"थोड़ा कम कर दीजिए\"},\n",
        "                {\"phrase\": \"Badhiya maal hai\", \"meaning\": \"It's good quality\", \"devanagari\": \"बढ़िया माल है\"}\n",
        "            ]\n",
        "\n",
        "            restaurant_phrases = [\n",
        "                {\"phrase\": \"Menu dikha dijiye\", \"meaning\": \"Please show me the menu\", \"devanagari\": \"मेनू दिखा दीजिए\"},\n",
        "                {\"phrase\": \"Thoda teekha hai\", \"meaning\": \"It's a bit spicy\", \"devanagari\": \"थोड़ा तीखा है\"},\n",
        "                {\"phrase\": \"Bill le aayiye\", \"meaning\": \"Please bring the bill\", \"devanagari\": \"बिल ले आइए\"}\n",
        "            ]\n",
        "\n",
        "            if \"market\" in context.lower():\n",
        "                return market_phrases[:top_k]\n",
        "            elif \"restaurant\" in context.lower():\n",
        "                return restaurant_phrases[:top_k]\n",
        "            else:\n",
        "                return market_phrases[:top_k]  # Default to market\n",
        "\n",
        "        # If RAG is available, extract phrases from retrieved examples\n",
        "        examples = self.retrieve_dialogue_examples(context, top_k=top_k)\n",
        "\n",
        "        phrases = []\n",
        "        for example in examples:\n",
        "            dialogue_turns = example.get(\"dialogue_turns\", [])\n",
        "\n",
        "            # Extract short phrases from dialogue turns\n",
        "            for turn in dialogue_turns:\n",
        "                text_roman = turn.get(\"text_roman\", \"\")\n",
        "                text_devanagari = turn.get(\"text_devanagari\", \"\")\n",
        "\n",
        "                # Look for short phrases (3-5 words)\n",
        "                words = text_roman.split()\n",
        "                if 3 <= len(words) <= 10:\n",
        "                    phrases.append({\n",
        "                        \"phrase\": text_roman,\n",
        "                        \"devanagari\": text_devanagari,\n",
        "                        \"meaning\": \"\"  # We would need translation for this\n",
        "                    })\n",
        "\n",
        "        # Return unique phrases, limited to top_k\n",
        "        unique_phrases = []\n",
        "        seen_phrases = set()\n",
        "\n",
        "        for phrase in phrases:\n",
        "            if phrase[\"phrase\"] not in seen_phrases:\n",
        "                seen_phrases.add(phrase[\"phrase\"])\n",
        "                unique_phrases.append(phrase)\n",
        "\n",
        "                if len(unique_phrases) >= top_k:\n",
        "                    break\n",
        "\n",
        "        # If we don't have enough phrases, add default ones\n",
        "        if len(unique_phrases) < top_k:\n",
        "            default_phrases = [\n",
        "                {\"phrase\": \"Kitne ka hai?\", \"meaning\": \"How much is it?\", \"devanagari\": \"कितने का है?\"},\n",
        "                {\"phrase\": \"Thoda kam kar dijiye\", \"meaning\": \"Please reduce it a little\", \"devanagari\": \"थोड़ा कम कर दीजिए\"},\n",
        "                {\"phrase\": \"Badhiya maal hai\", \"meaning\": \"It's good quality\", \"devanagari\": \"बढ़िया माल है\"}\n",
        "            ]\n",
        "\n",
        "            for phrase in default_phrases:\n",
        "                if phrase[\"phrase\"] not in seen_phrases and len(unique_phrases) < top_k:\n",
        "                    seen_phrases.add(phrase[\"phrase\"])\n",
        "                    unique_phrases.append(phrase)\n",
        "\n",
        "        return unique_phrases\n",
        "\n",
        "# LLM Functions\n",
        "def load_model():\n",
        "    \"\"\"Load the LLM with quantization.\"\"\"\n",
        "    print(f\"Loading model {MODEL_NAME}...\")\n",
        "\n",
        "    # Configure quantization\n",
        "    if USE_4BIT:\n",
        "        quantization_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_compute_dtype=torch.float16,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\"\n",
        "        )\n",
        "    else:\n",
        "        quantization_config = BitsAndBytesConfig(\n",
        "            load_in_8bit=True\n",
        "        )\n",
        "\n",
        "    # Load model and tokenizer\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        quantization_config=quantization_config,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "\n",
        "    # Set pad token if not set\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "def create_role_locked_prompt(scenario_type, rag_examples=None):\n",
        "    \"\"\"Create a prompt that strictly enforces the assistant's role, enhanced with RAG examples.\"\"\"\n",
        "    base_system = \"\"\"You are a Hindi language tutor demonstrating ONLY the {role} role in a {scenario_type} conversation.\n",
        "\n",
        "CRITICAL ROLE INSTRUCTIONS:\n",
        "1. You ONLY play the {role} - NEVER respond as the customer.\n",
        "2. The human user ALWAYS plays the customer.\n",
        "3. NEVER continue the conversation as the customer.\n",
        "4. NEVER put \"Customer:\" or similar labels in your responses.\n",
        "5. If you notice yourself starting to respond as the customer, STOP IMMEDIATELY.\n",
        "\n",
        "FORMAT REQUIREMENTS:\n",
        "1. First line: Short response in Roman Hindi (max 2 sentences)\n",
        "2. Second line: Same response in Devanagari script\n",
        "3. NOTHING ELSE.\n",
        "\n",
        "CONTENT GUIDELINES:\n",
        "1. Keep responses SHORT and PRACTICAL.\n",
        "2. Use authentic, everyday Hindi {scenario_type} language.\n",
        "3. Don't create elaborate stories or explanations.\n",
        "4. Use REALISTIC Hindi that would be spoken in a real {scenario_type}.\"\"\"\n",
        "\n",
        "    scenarios = {\n",
        "        \"market\": {\n",
        "            \"role\": \"shopkeeper\",\n",
        "            \"system\": base_system.format(role=\"shopkeeper\", scenario_type=\"market\"),\n",
        "            \"examples\": [\n",
        "                \"Namaste ji, kya chahiye aapko?\\nनमस्ते जी, क्या चाहिए आपको?\",\n",
        "                \"Haan ji, ye taza tamatar hai. Pachaas rupaye kilo.\\nहां जी, ये ताज़ा टमाटर हैं। पचास रुपये किलो।\"\n",
        "            ]\n",
        "        },\n",
        "        \"restaurant\": {\n",
        "            \"role\": \"waiter\",\n",
        "            \"system\": base_system.format(role=\"waiter\", scenario_type=\"restaurant\"),\n",
        "            \"examples\": [\n",
        "                \"Namaste ji, kya khaayenge aap?\\nनमस्ते जी, क्या खाएंगे आप?\",\n",
        "                \"Ji zaroor, paneer butter masala aur do roti. Kuchh aur?\\nजी ज़रूर, पनीर बटर मसाला और दो रोटी। कुछ और?\"\n",
        "            ]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    scenario_info = scenarios.get(scenario_type, scenarios[\"market\"])\n",
        "\n",
        "    # Add RAG examples if available\n",
        "    rag_content = \"\"\n",
        "    if rag_examples:\n",
        "        rag_content = \"\\n\\nREFERENCE EXAMPLES (use these for authentic Hindi expressions):\\n\"\n",
        "        for i, example in enumerate(rag_examples):\n",
        "            if \"dialogue_turns\" in example:\n",
        "                # Extract a few turns for examples\n",
        "                turns = example.get(\"dialogue_turns\", [])\n",
        "                if turns:\n",
        "                    rag_content += f\"Example {i+1}:\\n\"\n",
        "                    for j, turn in enumerate(turns[:3]):  # Limit to 3 turns\n",
        "                        speaker = turn.get(\"speaker\", \"\")\n",
        "                        text = turn.get(\"text_roman\", \"\")\n",
        "                        rag_content += f\"{speaker}: {text}\\n\"\n",
        "                    rag_content += \"\\n\"\n",
        "\n",
        "    return scenario_info[\"system\"] + rag_content\n",
        "\n",
        "def generate_initial_greeting(model, tokenizer, rag_system, scenario_type=\"market\"):\n",
        "    \"\"\"Generate just an initial greeting with strict role enforcement and RAG enhancement.\"\"\"\n",
        "    # Get relevant dialogue examples for this scenario\n",
        "    context_tags = None\n",
        "    if scenario_type == \"market\":\n",
        "        context_tags = [\"shopping\", \"market\", \"bazaar\"]\n",
        "    elif scenario_type == \"restaurant\":\n",
        "        context_tags = [\"food\", \"restaurant\"]\n",
        "\n",
        "    examples = []\n",
        "    if rag_system and not rag_system.dummy_mode:\n",
        "        examples = rag_system.retrieve_dialogue_examples(\n",
        "            query=f\"greeting in a {scenario_type}\",\n",
        "            top_k=2,\n",
        "            context_tags=context_tags\n",
        "        )\n",
        "\n",
        "\n",
        "    system_prompt = create_role_locked_prompt(scenario_type, examples)\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": f\"Start with a typical greeting a {scenario_type} {scenarios[scenario_type]['role']} would use. Keep it short and authentic. ONLY respond as the {scenarios[scenario_type]['role']}.\"}\n",
        "    ]\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=100,  # Short to avoid role confusion\n",
        "        do_sample=True,\n",
        "        temperature=TEMPERATURE,\n",
        "        repetition_penalty=1.2,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
        "\n",
        "    response = clean_response(response)\n",
        "\n",
        "    clear_gpu_memory()\n",
        "    return response\n",
        "\n",
        "def continue_dialogue(model, tokenizer, rag_system, user_input, scenario_type=\"market\", in_hindi=False):\n",
        "    \"\"\"Continue the conversation with enforced role boundaries and RAG enhancement.\"\"\"\n",
        "    # Get relevant dialogue examples for this input\n",
        "    examples = []\n",
        "    if rag_system and not rag_system.dummy_mode:\n",
        "        context_tags = None\n",
        "        if scenario_type == \"market\":\n",
        "            context_tags = [\"shopping\", \"market\", \"bazaar\"]\n",
        "        elif scenario_type == \"restaurant\":\n",
        "            context_tags = [\"food\", \"restaurant\"]\n",
        "\n",
        "        examples = rag_system.retrieve_dialogue_examples(\n",
        "            query=user_input,\n",
        "            top_k=2,\n",
        "            context_tags=context_tags\n",
        "        )\n",
        "\n",
        "\n",
        "    system_prompt = create_role_locked_prompt(scenario_type, examples)\n",
        "\n",
        "\n",
        "    if in_hindi:\n",
        "        system_prompt += \"\\n\\nNOTE: The customer is responding in Hindi, which is excellent! Encourage them by acknowledging their Hindi usage in your response.\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": f\"The customer says: \\\"{user_input}\\\"\\n\\nRespond ONLY as the {scenarios[scenario_type]['role']} in short, simple Hindi (both Roman and Devanagari). NEVER respond as the customer. Keep your response brief and practical.\"}\n",
        "    ]\n",
        "\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=100,  # Short to avoid role confusion\n",
        "        do_sample=True,\n",
        "        temperature=TEMPERATURE,\n",
        "        repetition_penalty=1.3,  # Higher to avoid repetitive patterns that might cause role confusion\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
        "\n",
        "    # Clean up response to ensure proper format\n",
        "    response = clean_response(response)\n",
        "\n",
        "    clear_gpu_memory()\n",
        "    return response, examples\n",
        "\n",
        "def clean_response(response):\n",
        "    \"\"\"Clean up the response to ensure proper format and remove role confusion.\"\"\"\n",
        "    # Remove any \"Shopkeeper:\" or \"Waiter:\" or \"Customer:\" labels\n",
        "    response = re.sub(r'(Shopkeeper|Waiter|Customer|Assistant):\\s*', '', response)\n",
        "\n",
        "    lines = response.split('\\n')\n",
        "    cleaned_lines = []\n",
        "\n",
        "    roman_line = \"\"\n",
        "    devanagari_line = \"\"\n",
        "\n",
        "    # Simple heuristic - first non-empty line is Roman, second is Devanagari\n",
        "    for line in lines:\n",
        "        if line.strip():\n",
        "            if not roman_line:\n",
        "                roman_line = line.strip()\n",
        "            elif not devanagari_line:\n",
        "                devanagari_line = line.strip()\n",
        "                break\n",
        "\n",
        "    # If we found both lines, use them\n",
        "    if roman_line and devanagari_line:\n",
        "        return f\"{roman_line}\\n{devanagari_line}\"\n",
        "\n",
        "    # If we didn't find a clear structure, just return the cleaned original\n",
        "    return response\n",
        "\n",
        "def is_hindi(text):\n",
        "    \"\"\"Check if the text contains Hindi (either in Devanagari or romanized).\"\"\"\n",
        "    # Check for Devanagari characters\n",
        "    devanagari_pattern = re.compile(r'[\\u0900-\\u097F]')\n",
        "    if devanagari_pattern.search(text):\n",
        "        return True\n",
        "\n",
        "    # Check for likely romanized Hindi words\n",
        "    hindi_romanized_words = [\n",
        "        'namaste', 'dhanyavad', 'theek', 'haan', 'nahi', 'kya', 'aap', 'mai', 'tum',\n",
        "        'kitna', 'rupaye', 'paisa', 'khana', 'pani', 'chai', 'acha', 'bahut', 'thoda'\n",
        "    ]\n",
        "\n",
        "    text_lower = text.lower()\n",
        "    for word in hindi_romanized_words:\n",
        "        if word in text_lower:\n",
        "            return True\n",
        "\n",
        "    return False\n",
        "\n",
        "def get_encouragement_message():\n",
        "    \"\"\"Get a random encouragement message to use Hindi.\"\"\"\n",
        "    import random\n",
        "    return random.choice(ENCOURAGEMENT_PHRASES)\n",
        "\n",
        "# Scenario info\n",
        "scenarios = {\n",
        "    \"market\": {\n",
        "        \"role\": \"shopkeeper\",\n",
        "        \"name\": \"Market\",\n",
        "        \"description\": \"Practice buying items, haggling prices, and asking about products in a typical Indian market.\"\n",
        "    },\n",
        "    \"restaurant\": {\n",
        "        \"role\": \"waiter\",\n",
        "        \"name\": \"Restaurant\",\n",
        "        \"description\": \"Practice ordering food, asking about dishes, and handling restaurant interactions in Hindi.\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Main conversation function\n",
        "def run_hindi_practice(scenario_type=\"market\"):\n",
        "    print(\"Initializing Hindi Language Learning System...\")\n",
        "\n",
        "    # Initialize RAG system\n",
        "    rag_system = None\n",
        "    try:\n",
        "        print(\"Setting up RAG system...\")\n",
        "        rag_system = HindiLearningRAG()\n",
        "\n",
        "        # Try to load existing vector store\n",
        "        if not rag_system.load_vector_store():\n",
        "            if os.path.exists(RAG_DATA_PATH):\n",
        "                print(f\"Found document data at {RAG_DATA_PATH}, loading...\")\n",
        "                if rag_system.load_documents(RAG_DATA_PATH):\n",
        "                    rag_system.create_vector_store()\n",
        "            else:\n",
        "                print(f\"RAG data not found at {RAG_DATA_PATH}. Using basic mode.\")\n",
        "                rag_system.dummy_mode = True\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing RAG system: {e}\")\n",
        "        rag_system = HindiLearningRAG(dummy_mode=True)\n",
        "\n",
        "    # Load LLM model\n",
        "    print(f\"Loading model for {scenario_type} conversation practice...\")\n",
        "    model, tokenizer = load_model()\n",
        "\n",
        "    # Generate initial greeting\n",
        "    greeting = generate_initial_greeting(model, tokenizer, rag_system, scenario_type)\n",
        "\n",
        "    # Get useful Hindi phrases for this scenario\n",
        "    useful_phrases = rag_system.get_hindi_phrases_for_context(f\"{scenario_type} conversation\", top_k=3)\n",
        "\n",
        "    # Print setup information\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"HINDI {scenario_type.upper()} PRACTICE\")\n",
        "    print(f\"{'='*50}\\n\")\n",
        "\n",
        "    # Print Hindi phrases suggestion\n",
        "    print(\"Useful Hindi phrases for this scenario:\")\n",
        "    for phrase in useful_phrases:\n",
        "        print(f\"• {phrase['phrase']} - {phrase.get('devanagari', '')}\")\n",
        "        if phrase.get('meaning'):\n",
        "            print(f\"  ({phrase['meaning']})\")\n",
        "    print(f\"\\n{'-'*50}\")\n",
        "\n",
        "    # Start conversation\n",
        "    print(\"Tutor (as shopkeeper/waiter):\")\n",
        "    print(greeting)\n",
        "    print(\"\\n\" + \"-\"*50)\n",
        "\n",
        "    user_used_hindi = False\n",
        "    hindi_encouragement_count = 0\n",
        "\n",
        "    while True:\n",
        "        if not user_used_hindi and hindi_encouragement_count % 2 == 0:\n",
        "            print(f\"\\n💡 {get_encouragement_message()}\")\n",
        "\n",
        "        user_input = input(\"You (as customer): \")\n",
        "\n",
        "        if user_input.lower() in ['exit', 'quit', 'bye']:\n",
        "            print(\"\\nConversation ended. Dhanyavaad! (Thank you!)\")\n",
        "            break\n",
        "\n",
        "        current_input_in_hindi = is_hindi(user_input)\n",
        "        if current_input_in_hindi:\n",
        "            user_used_hindi = True\n",
        "\n",
        "        response, examples = continue_dialogue(\n",
        "            model, tokenizer, rag_system, user_input,\n",
        "            scenario_type, in_hindi=current_input_in_hindi\n",
        "        )\n",
        "\n",
        "        print(\"\\nTutor (as shopkeeper/waiter):\")\n",
        "        print(response)\n",
        "\n",
        "        if examples and False:\n",
        "            print(\"\\nRetrieved examples:\")\n",
        "            for i, example in enumerate(examples):\n",
        "                print(f\"Example {i+1}: {example.get('scene_description', '')}\")\n",
        "\n",
        "        print(\"\\n\" + \"-\"*50)\n",
        "\n",
        "        if not user_used_hindi:\n",
        "            hindi_encouragement_count += 1\n",
        "\n",
        "\n",
        "    del model\n",
        "    clear_gpu_memory()\n",
        "\n",
        "# Usage example\n",
        "if __name__ == \"__main__\":\n",
        "    scenario = \"market\"  # or \"restaurant\"\n",
        "    run_hindi_practice(scenario)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b7346df94c3b4cc8a9c55b5717946e5c",
            "21da0bea3c8e49b2b353bb07424dc0b5",
            "c90bcd1f77084888a723f6bf7ebf79a1",
            "c9ee4a53a7c94407b1ebb08cf37b479c",
            "336005438c6f498dbcd9f2e21f218e4f",
            "9df7f43743904f56a9fab8d575f333c6",
            "a7295a8f4ad241a7bd69fa153210eebd",
            "6a5797be6bf54674a06e229e662c9dc6",
            "e00d4b38754c4d57ac0a5628975c65d9",
            "445db4e443b045f8a1c0d02c032b49ca",
            "cfe6f20552c14b68a99497c0bdebb1ea"
          ]
        },
        "id": "R5Z3TWIRLUOh",
        "outputId": "7ec12c64-f834-4283-c30c-d70280e2d1be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing Hindi Language Learning System...\n",
            "Setting up RAG system...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-763aa6214d41>:87: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  self.embeddings = HuggingFaceEmbeddings(\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name ai4bharat/indic-bert. Creating a new one with mean pooling.\n",
            "ERROR:__main__:Error loading vector store: The de-serialization relies loading a pickle file. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine.You will need to set `allow_dangerous_deserialization` to `True` to enable deserialization. If you do this, make sure that you trust the source of the data. For example, if you are loading a file that you created, and know that no one else has modified the file, then this is safe to do. Do not set this to `True` if you are loading a file from an untrusted source (e.g., some random site on the internet.).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found document data at processed_hindi_dialogues.json, loading...\n",
            "Loading model for market conversation practice...\n",
            "Loading model Cognitive-Lab/LLama3-Gaja-Hindi-8B-v0.1...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b7346df94c3b4cc8a9c55b5717946e5c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "HINDI MARKET PRACTICE\n",
            "==================================================\n",
            "\n",
            "Useful Hindi phrases for this scenario:\n",
            "• Kitne ka hai? - कितने का है?\n",
            "  (How much is it?)\n",
            "• Thoda kam kar dijiye - थोड़ा कम कर दीजिए\n",
            "  (Please reduce it a little)\n",
            "• Badhiya maal hai - बढ़िया माल है\n",
            "  (It's good quality)\n",
            "\n",
            "--------------------------------------------------\n",
            "Tutor (as shopkeeper/waiter):\n",
            "Namaste bhaiya, kaise ho? Aapka kaam kaisa chal raha hai?\n",
            "नमस्ते भैया, कैसे हो? आपका काम कैसा चल रहा है?user\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "💡 Even simple Hindi words help you learn! / सरल हिंदी शब्द भी आपको सीखने में मदद करते हैं!\n",
            "You (as customer): काम तो काम है, मुझे कुछ केले चाहिए, उनकी कीमत क्या है?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Tutor (as shopkeeper/waiter):\n",
            "Keliyon ki khaasiyat hai ye sabse acchi hain aur unka price Rs.40 per dozen hota hai.\n",
            "खरीदने वालों से, हमारे पास सबसे अच्छा और लोकप्रिय फल होते हैं जिन्हें आप खरीद सकते हैं, और इसका दाम एक दर्जन तक ₹४० रुपये तक रहता है।user\n",
            "\n",
            "--------------------------------------------------\n",
            "You (as customer): हा हा, तुम सही कह रहे हो। मैं आधा दर्जन लूंगा।\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Tutor (as shopkeeper/waiter):\n",
            "Roman Hindi Response\n",
            "अच्छे, कौन सा प्रकार के चॉकलेट्स चाहिए?\n",
            "\n",
            "--------------------------------------------------\n",
            "You (as customer): नहीं, मुझे आधा दर्जन केले चाहिए, मेरे पास अभी सिर्फ़ 20 रुपये हैं\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Tutor (as shopkeeper/waiter):\n",
            "Kya pata hai kuchh bhi ho sakta tha? Aapko abhi ek doosra packet mil jayega aur wohi price mein hi dena hoga ₹40 tak.\n",
            "क्या, कुछ भी हो सकता था। आप एक और बॉक्स लेंगे, वैस्ट अमाउंट पर देने जाने। ₹40 तक।user\n",
            "\n",
            "--------------------------------------------------\n",
            "You (as customer): Here you go 40 Rs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tutor (as shopkeeper/waiter):\n",
            "Tumne kaun sa product chune? ₹40 ke liye kya mil raha hai?\n",
            "तुमने कौन सा प्रोडक्ट चुने, ₹४० की लिए क्या मिल रहा है?user\n",
            "\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-763aa6214d41>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m     \u001b[0mscenario\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"market\"\u001b[0m  \u001b[0;31m# or \"restaurant\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 684\u001b[0;31m     \u001b[0mrun_hindi_practice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscenario\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-763aa6214d41>\u001b[0m in \u001b[0;36mrun_hindi_practice\u001b[0;34m(scenario_type)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n💡 {get_encouragement_message()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m         \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You (as customer): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'exit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'quit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bye'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b5bSYzoQT91b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}